{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trickng ResNet18 Image Classifier Using LLM Suggested Image Perturbations"
      ],
      "metadata": {
        "id": "ZSxEHJXKt73P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download imagenette dataset\n",
        "# imagenette is subset of 10 classes from full ImageNet dataset\n",
        "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz"
      ],
      "metadata": {
        "id": "ZaBwxDUke7es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpack dataset in current Colab environment\n",
        "!tar -xvzf imagenette2.tgz"
      ],
      "metadata": {
        "id": "aQZZgpaCfCtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWCVNMHLJrmB"
      },
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install missing packages from main implementation\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -q transformers accelerate bitsandbytes torch\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "dedAlOBJMggL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, datasets, models\n",
        "import clip\n"
      ],
      "metadata": {
        "id": "NZw6LALN1K0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "hEaGvvpWxIp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test images\n",
        "# url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "# try: urllib.URLopener().retrieve(url, filename)\n",
        "# except: urllib.request.urlretrieve(url, filename)\n",
        "# img = Image.open(filename)"
      ],
      "metadata": {
        "id": "7197SUYYkuXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with token\n",
        "# Retrieves token: Assumes that everyone needs their own hugging face account and add token to colab env\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Load model with authentication\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_token,\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    ),\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "8VEOUDsLMz5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test that the model and image loading worked\n",
        "def generate_response(prompt, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = llm_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "DgVVyicmGrQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPTING"
      ],
      "metadata": {
        "id": "Jnbvbemi1sz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perturbations_prompt = \"Give locations, size, and type of perturbations to be done to an image. The image size is 1920 by 1080 and the maximum perturbation size is 100 by 100. Give a list of 10 perturbations which specify pixel location, size of the perturbation patch, channel an integer between 0 and 3, and type of perturbation. Respond only in JSON format with no explainations.\"\n",
        "\n",
        "# perturbation_examples = \"\"\"  {\"location\": [20, 30], \"size\": [10, 10], \"channel\": 0, \"type\": \"gaussian_noise\"},\n",
        "#   {\"location\": [50, 60], \"size\": [8, 8], \"channel\": 0, \"type\": \"blur\"},\n",
        "#   {\"location\": [70, 20], \"size\": [6, 6], \"channel\": 1, \"type\": \"occlusion\"},\n",
        "#   {\"location\": [10, 10], \"size\": [9, 9], \"channel\": 2, \"type\": \"brightness_increase\"},\n",
        "#   {\"location\": [80, 40], \"size\": [7, 7], \"channel\": 0, \"type\": \"contrast_decrease\"},\n",
        "#   {\"location\": [30, 70], \"size\": [10, 10], \"channel\": 1, \"type\": \"salt_and_pepper_noise\"},\n",
        "#   {\"location\": [60, 15], \"size\": [5, 5], \"channel\": 2, \"type\": \"motion_blur\"},\n",
        "#   {\"location\": [25, 85], \"size\": [6, 6], \"channel\": 1, \"type\": \"color_shift\"},\n",
        "#   {\"location\": [90, 90], \"size\": [10, 10], \"channel\": 2, \"type\": \"sharpen\"},\n",
        "#   {\"location\": [40, 50], \"size\": [7, 7], \"channel\": 0, \"type\": \"grayscale\"}\n",
        "#   \"\"\"\n",
        "\n",
        "# ranks = [0.9,1,0.2,0.3,0.1,0.5,0,0.2,0.2,0.8]\n",
        "\n"
      ],
      "metadata": {
        "id": "XxcYBxiShWLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perturbations_prompt = \"Give types of perturbations to be done to an image. The image size is 256 by 256.\\\n",
        "Give a list of up to 10 perturbations where operations are one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "Rotate requires an angle parameter.\\\n",
        "adjust_brightness requires a factor parameter.\\\n",
        "blur_patch requires center, radius, and sigma parameters.\\\n",
        "add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "translate requires x_shift and y_shift parameters.\\\n",
        "Respond only in JSON format with no explainations.\"\n",
        "\n",
        "perturbation_examples = \"\"\"\n",
        "    [{\n",
        "            \"operation\": \"rotate\",\n",
        "            \"angle\": 15\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"adjust_brightness\",\n",
        "            \"factor\": 1.5\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"blur_patch\",\n",
        "            \"center\": [80, 60],\n",
        "            \"radius\": 20,\n",
        "            \"sigma\": 5.0\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_stripe_noise\",\n",
        "            \"orientation\": \"horizontal\",\n",
        "            \"stripe_width\": 10,\n",
        "            \"intensity\": 0.1,\n",
        "            \"location\": 0.3\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_patch\",\n",
        "            \"location\": [50, 50],\n",
        "            \"size\": [40, 40],\n",
        "            \"type\": \"noise\"\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_patch\",\n",
        "            \"location\": [120, 120],\n",
        "            \"size\": [30, 30],\n",
        "            \"type\": \"color\",\n",
        "            \"color\": [255, 0, 0]\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"translate\",\n",
        "            \"x_shift\": 20,\n",
        "            \"y_shift\": 30\n",
        "    }]\"\"\""
      ],
      "metadata": {
        "id": "MVNe8F58cbIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original prompt is below:\n",
        "# f\"Give types of perturbations to be done to an image. The image size is {size[0]} by {size[1]}.\\\n",
        "# Give a list of up to 10 perturbations where operations are one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "# Rotate requires an angle parameter.\\\n",
        "# adjust_brightness requires a factor parameter.\\\n",
        "# blur_patch requires center, radius, and sigma parameters.\\\n",
        "# add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "# add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "# translate requires x_shift and y_shift parameters.\\\n",
        "# Respond only in JSON format with no explainations.\""
      ],
      "metadata": {
        "id": "0GRoonFfj8tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_new_perturbations_prompt(prev_pert = perturbation_examples, ranks = [], size = [224,224], label  = \"\", labels = []):\n",
        "  init_prompt = f\"Give a list of up to 10 image perturbations that can trick ResNet's image classifier into misclassifying the perturbed image while keeping the perturbed image as close to the original one as possible. The image is of a \" + label + \". Strictly follow these guidelines:\"\n",
        "  base_prompt = f\"\\\n",
        "Operations are exclusively one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "The image size is {size[0]} by {size[1]}.\\\n",
        "Rotate requires an angle parameter.\\\n",
        "adjust_brightness requires a factor parameter.\\\n",
        "blur_patch requires center, radius, and sigma parameters.\\\n",
        "add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "translate requires x_shift and y_shift parameters.\\\n",
        "Strictly respond only in JSON format with no explanations.\"\n",
        "\n",
        "  perturbations_prompt = f\"{init_prompt}{base_prompt}\"\n",
        "  if ranks == []:\n",
        "    prompt = init_prompt + \"\\Examples: \" + str(prev_pert) + \"Generate new perturbations strictly based on these rules:\" + base_prompt\n",
        "  else:\n",
        "    prompt = \"Previous perturbations were: \" + str(prev_pert) + \" and had a score of: \" + str(ranks) + \" on the following actual labels \" + str(labels) + \".\\nStricly follow these rules to give a list of up to 10 perturbations to increase that loss value:\\n\" + base_prompt\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "qRWRV35MmsiH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test = generate_new_perturbations_prompt()\n",
        "# print(test)"
      ],
      "metadata": {
        "id": "6H1PylBoqawr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Select only the json list from the response\n",
        "def extract_text_between_brackets(text):\n",
        "  text = text.split(\"Strictly respond only in JSON format with no explanations.\")[1] # Ignore example json list\n",
        "  text = text.replace(' ', '')\n",
        "  text = text.replace('\\n', '')\n",
        "  text = text.replace('\\\\','')\n",
        "  if len(re.findall(r\"\\[\\{(.*?)\\}\\]\", text, re.DOTALL)) == 0: # If no full JSON exists return empty list\n",
        "    return \"[]\"\n",
        "  return \"[{\"+re.findall(r\"\\[\\{(.*?)\\}\\]\", text, re.DOTALL)[0]+\"}]\""
      ],
      "metadata": {
        "id": "zMfWJmZVtvrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = generate_response(test, 400)\n",
        "# print(extract_text_between_brackets(response))\n",
        "# commands = json.loads(extract_text_between_brackets(response))"
      ],
      "metadata": {
        "id": "3WIN0m5duWxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERTURBATOR"
      ],
      "metadata": {
        "id": "zLuNRYZ4jwWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_image(image, angle):\n",
        "    return image.rotate(angle)\n",
        "\n",
        "def adjust_brightness(image, factor):\n",
        "    enhancer = ImageEnhance.Brightness(image)\n",
        "    return enhancer.enhance(factor)\n",
        "\n",
        "def blur_patch(image, center, radius, sigma):\n",
        "    img_np = np.array(image)\n",
        "    (h, w, _) = img_np.shape\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "    cv2.circle(mask, tuple(center), radius, 255, -1)\n",
        "\n",
        "    blurred = cv2.GaussianBlur(img_np, (0, 0), sigma)\n",
        "\n",
        "    mask = mask[:, :, np.newaxis] / 255.0\n",
        "    output = img_np * (1 - mask) + blurred * mask\n",
        "    output = np.clip(output, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return Image.fromarray(output)\n",
        "\n",
        "def add_stripe_noise(image, orientation, stripe_width, intensity, location=0):\n",
        "    img_np = np.array(image).astype(np.float32) / 255.0\n",
        "    noise = np.random.uniform(-intensity, intensity, img_np.shape)\n",
        "\n",
        "    mask = np.zeros_like(img_np)\n",
        "    H, W, _ = img_np.shape\n",
        "\n",
        "    if orientation == \"horizontal\":\n",
        "        # Horizontal stripe centered at given y-location\n",
        "        y_center = int(location * H)\n",
        "        y_start = max(0, y_center - stripe_width // 2)\n",
        "        y_end = min(H, y_center + stripe_width // 2)\n",
        "        mask[y_start:y_end, :, :] = 1\n",
        "    elif orientation == \"vertical\":\n",
        "        # Vertical stripe centered at given x-location\n",
        "        x_center = int(location * W)\n",
        "        x_start = max(0, x_center - stripe_width // 2)\n",
        "        x_end = min(W, x_center + stripe_width // 2)\n",
        "        mask[:, x_start:x_end, :] = 1\n",
        "\n",
        "    noisy_img = np.clip(img_np + noise * mask, 0, 1)\n",
        "    noisy_img = (noisy_img * 255).astype(np.uint8)\n",
        "    return Image.fromarray(noisy_img)\n",
        "\n",
        "def add_patch(image, location, size, type_=\"noise\", color=None):\n",
        "    img_np = np.array(image)\n",
        "\n",
        "    patch = None\n",
        "    if type_ == \"noise\":\n",
        "        patch = np.random.randint(0, 256, (size[1], size[0], 3), dtype=np.uint8)\n",
        "    elif type_ == \"color\" and color is not None:\n",
        "        patch = np.ones((size[1], size[0], 3), dtype=np.uint8) * np.array(color, dtype=np.uint8)\n",
        "\n",
        "    x, y = location  # Now input is exact (x, y)\n",
        "\n",
        "    # Boundary check\n",
        "    H, W, _ = img_np.shape\n",
        "    x = max(0, min(x, W - size[0]))\n",
        "    y = max(0, min(y, H - size[1]))\n",
        "\n",
        "    img_np[y:y+size[1], x:x+size[0]] = patch\n",
        "\n",
        "    return Image.fromarray(img_np)\n",
        "\n",
        "def translate_image(image, x_shift, y_shift):\n",
        "    img_np = np.array(image)\n",
        "    (h, w) = img_np.shape[:2]\n",
        "\n",
        "    M = np.float32([[1, 0, x_shift], [0, 1, y_shift]])\n",
        "    shifted = cv2.warpAffine(img_np, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
        "\n",
        "    return Image.fromarray(shifted)\n"
      ],
      "metadata": {
        "id": "coVWyHoUjyh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_action(image, action_dict):\n",
        "    operation = action_dict[\"operation\"]\n",
        "\n",
        "    if operation == \"rotate\":\n",
        "        return rotate_image(image, angle=action_dict[\"angle\"])\n",
        "    elif operation == \"adjust_brightness\":\n",
        "        return adjust_brightness(image, factor=action_dict[\"factor\"])\n",
        "    elif operation == \"blur_patch\":\n",
        "        return blur_patch(\n",
        "            image,\n",
        "            center=action_dict[\"center\"],\n",
        "            radius=action_dict[\"radius\"],\n",
        "            sigma=action_dict[\"sigma\"]\n",
        "        )\n",
        "    elif operation == \"add_stripe_noise\":\n",
        "        return add_stripe_noise(\n",
        "            image,\n",
        "            orientation=action_dict[\"orientation\"],\n",
        "            stripe_width=action_dict[\"stripe_width\"],\n",
        "            intensity=action_dict[\"intensity\"],\n",
        "            location=action_dict.get(\"location\", 0)\n",
        "        )\n",
        "    elif operation == \"add_patch\":\n",
        "        return add_patch(\n",
        "            image,\n",
        "            location=action_dict[\"location\"],\n",
        "            size=action_dict[\"size\"],\n",
        "            type_=action_dict.get(\"type\", \"noise\"),\n",
        "            color=action_dict.get(\"color\")\n",
        "        )\n",
        "    elif operation == \"translate\":\n",
        "        return translate_image(\n",
        "            image,\n",
        "            x_shift=action_dict[\"x_shift\"],\n",
        "            y_shift=action_dict[\"y_shift\"]\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown operation: {operation}\")\n"
      ],
      "metadata": {
        "id": "pUqsJ-H-jyfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image, title=\"Image\"):\n",
        "    plt.imshow(np.array(image))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# for command in commands:\n",
        "#   perturbed_img = apply_action(img, command)\n",
        "#   show_image(perturbed_img, title=f\"Perturbation: {command['operation']}\")"
      ],
      "metadata": {
        "id": "ragD8kC1jyc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESNET"
      ],
      "metadata": {
        "id": "3lBQj-a_1q-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "# # or any of these variants\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
        "# resnet_model.eval()"
      ],
      "metadata": {
        "id": "pKYCuqVT1qhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ImageNet labels\n",
        "# !wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
      ],
      "metadata": {
        "id": "l4WPE7jR1zxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image, model, data):\n",
        "  # model should be preloaded fine-tuned model from earlier\n",
        "  # data is the current subset of the ImageNette val dataset\n",
        "  # image is the current image from the test set\n",
        "  # sample execution (requires torchvision)\n",
        "  # this could def be implemented a lot nicer\n",
        "  input_image = image\n",
        "\n",
        "  preprocess = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Resize(224),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "  # Normalize input image\n",
        "  input_tensor = preprocess(input_image)\n",
        "\n",
        "  # create a mini-batch as expected by the model\n",
        "  input_batch = input_tensor.unsqueeze(0)\n",
        "  input_batch = input_batch.to(device)\n",
        "  print(device)\n",
        "\n",
        "  # Make prediction on label\n",
        "  with torch.no_grad():\n",
        "      output = model(input_batch)\n",
        "  # Tensor of shape 10, with confidence scores over CIFAR10's 10 classes\n",
        "  # print(output[0])\n",
        "  # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "  probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "  # print(probabilities)\n",
        "\n",
        "  # Read the categories\n",
        "  categories = data.dataset.classes\n",
        "\n",
        "  # Show top category per image\n",
        "  top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "  # for i in range(top5_prob.size(0)):\n",
        "  #     print(categories[top5_catid[i]], top5_prob[i].item())\n",
        "\n",
        "  # Return top prediction\n",
        "  return categories[top5_catid[0]], top5_prob[0].item(), probabilities"
      ],
      "metadata": {
        "id": "_66lg5XT13wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img, label = test[14]\n",
        "# c, s, probabilities = classify_image(img, model=model, data=test)\n",
        "# c"
      ],
      "metadata": {
        "id": "rhdsRVIY21z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP SIMILARITY"
      ],
      "metadata": {
        "id": "VEhL7MfF-aE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize clip comparison objects\n",
        "# clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "tPRWVcXY-bXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_distance(image1, image2, clip_model, clip_preprocess):\n",
        "  clip_model.to(device)\n",
        "  cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "  image1_preprocess = clip_preprocess(image1).unsqueeze(0).to(device)\n",
        "  image1_features = clip_model.encode_image( image1_preprocess)\n",
        "\n",
        "  image2_preprocess = clip_preprocess(image2).unsqueeze(0).to(device)\n",
        "  image2_features = clip_model.encode_image( image2_preprocess)\n",
        "\n",
        "  similarity = cos(image1_features[0],image2_features[0]).item()\n",
        "  return 1 - (similarity+1)/2\n",
        "\n",
        "\n",
        "def image_distance2(image1, image2):\n",
        "  if image1.shape != image2.shape:\n",
        "      raise ValueError(\"Images must have the same shape\")\n",
        "  return np.linalg.norm(image1.astype(float) - image2.astype(float))\n"
      ],
      "metadata": {
        "id": "YG_HS-HUALJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity_score = image_distance(img, perturbed_img, clip_model, clip_preprocess)\n",
        "# similarity_score"
      ],
      "metadata": {
        "id": "0kAcGkQr_6hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOSS"
      ],
      "metadata": {
        "id": "0NEbxmtvE1C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(label, probabilities, similarity):\n",
        "  c = torch.tensor([0.5], device=device) # Hyperparameter\n",
        "  loss = torch.nn.CrossEntropyLoss()\n",
        "  loss = loss(label, probabilities)\n",
        "  return loss - similarity * c"
      ],
      "metadata": {
        "id": "b4tTsXmbE2Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one_hot_encoding_of_label = torch.zeros(10, device=device)\n",
        "# one_hot_encoding_of_label[probabilities.argmax()] = 1\n",
        "# loss_func(one_hot_encoding_of_label, probabilities, similarity_score)"
      ],
      "metadata": {
        "id": "fgg7s_MIE19a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUDO CODE FOR POSSIBLE EXECUTION CYCLE?"
      ],
      "metadata": {
        "id": "H3-IpBoF5b--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing to get images that resnet classifies correctly\n",
        "\n",
        "# prompt = generate_new_perturbations_prompt() ##First Pass\n",
        "# for epochs\n",
        "#   for image, label in images: Should probably be in batch form...\n",
        "#     response = generate_response(prompt, 400) ##First Pass\n",
        "#     json_list = extract_text_between_brackets(response) ##Done\n",
        "#     new_image = perturbator(image, json_list) ##Done\n",
        "#     _, _, probabilities = classify_image(new_image, model=model, data=test) ##Done\n",
        "#     similarity_score = image_distance(image, new_image, clip_model, clip_preprocess) ##Done\n",
        "#     loss = loss_func(label, probabilities, similarity_score) ##Done\n",
        "#     prompt = generate_new_perturbations_prompt(json_list, loss) ##First Pass"
      ],
      "metadata": {
        "id": "nnT9kkli5Tlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trick Pre-trained ResNet18 Image Classifier\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cSZ7kXNR0iuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "-iwy7L3ihhJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load flowers dataset and ResNet model\n",
        "# Normalize current dataset to specifics of original ImageNet dataset to stabilize and speed up learning\n",
        "# Reseize images to match previous implementation\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Resize((224,224)),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                      std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# Load dataset\n",
        "# Only need test since we are running inferences\n",
        "# test = datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "test = datasets.Imagenette(root='./data', split='val', download=True)\n",
        "\n",
        "# Load ResNet model\n",
        "# For scalability, we will only use the 18-layer version as its the smallest\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "# Replace the final fully connected layer to output 10 classes\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 10)\n",
        "\n",
        "# Map model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Initialize clip comparison objects\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "qEIuKYiw0oRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "adGDC10bh5Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load flowers dataset and ResNet model\n",
        "# Normalize current dataset to specifics of original ImageNet dataset to stabilize and speed up learning\n",
        "# Reseize images to match previous implementation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                     std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "# Only need test since we are running inferences\n",
        "# Apply the transform directly to the dataset\n",
        "# Load val to get baseline accuracy score from model\n",
        "test_full = datasets.Imagenette(root='./data', split='val', download=True)\n",
        "val  = datasets.Imagenette(root='./data', split='val', download=True, transform=transform)\n",
        "\n",
        "# take subset of test datset\n",
        "test = torch.utils.data.Subset(test_full,\n",
        "                               np.random.choice(len(test_full), 50,\n",
        "                                                replace=False))\n",
        "\n",
        "# Load ResNet model\n",
        "# For scalability, we will only use the 18-layer version as its the smallest\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "# Replace the final fully connected layer to output 10 classes\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 10)\n",
        "# Load pretrained checkpoint\n",
        "model.load_state_dict(torch.load(\"./resnet18_ImageNette.pth\"))\n",
        "\n",
        "# Map model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Initialize clip comparison objects\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Make test loader from here\n",
        "# The DataLoader will now receive tensors from the transformed dataset\n",
        "val_loader = torch.utils.data.DataLoader(val, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "hp4cw_aQh8xR",
        "outputId": "4addf1bd-959b-4079-e952-b2b5d42c309c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get baseline form running validation set through ResNet18\n",
        "correct, total = 0, 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Iterate using the val_loader to get batched data\n",
        "    for images, labels in val_loader:\n",
        "        # Move the batch to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device) # Also move labels if you plan to use them later\n",
        "\n",
        "        # The val_loader already provides 4D tensors (batch_size, channels, height, width)\n",
        "        pred = model(images)\n",
        "        _, predicted = pred.max(1)\n",
        "        total += labels.size(0) # Use labels.size(0) to get the batch size\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "baseline_accuracy = 100 * (correct / total)\n",
        "print(f\"Baseline Accuracy: {baseline_accuracy}%\")"
      ],
      "metadata": {
        "id": "epIzYgv1ypeN",
        "outputId": "b16a20ea-dc5e-428c-ff37-bd2189e5bf19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 97.98726114649682%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_operations = ['rotate', 'adjust_brightness', 'blur_patch',\n",
        "                    'add_stripe_noise', 'add_patch', 'translate']\n",
        "epochs = 3\n",
        "# Set size to 224 to match dataset transformation and ResNet expectations\n",
        "# pass size as list to match prompt implementation\n",
        "\n",
        "# Set model to evaluation mode for inferences\n",
        "model.eval()\n",
        "\n",
        "# Make counter variables for accuracy calculation later\n",
        "correct = 0\n",
        "current_img = 0\n",
        "\n",
        "misclassified_images = []\n",
        "misclassified_true_images = []\n",
        "perturbations = []\n",
        "cur_loss = []\n",
        "true_labels  = []\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for image_idx, (image, label) in enumerate(test):\n",
        "\n",
        "  true_label = str(test.dataset.classes[label]).lower()\n",
        "  cur_PIL = image\n",
        "\n",
        "  for i in range(epochs):\n",
        "    if i == 0 and image_idx == 0:\n",
        "      prompt = generate_new_perturbations_prompt(size=  cur_PIL.size)\n",
        "    else:\n",
        "      prompt = generate_new_perturbations_prompt(perturbations[-9:], cur_loss[-9:], perturbed_img.size, true_label, true_labels[-9:])\n",
        "    print(f\"STARTING EPOCH {i} FOR IMAGE #{image_idx}\")\n",
        "\n",
        "    response = generate_response(prompt, 400)\n",
        "    try:\n",
        "      json_list = json.loads(extract_text_between_brackets(response))\n",
        "    except json.JSONDecodeError:\n",
        "      json_list = []\n",
        "\n",
        "    loss = float(0)\n",
        "    perturbed_img = cur_PIL\n",
        "\n",
        "    if json_list != \"[]\":\n",
        "      remove_indices = []\n",
        "      print(\"Applying perturbations...\")\n",
        "      perturbation_count = 0\n",
        "      for command in json_list:\n",
        "        print(f\"apply perturbation #{perturbation_count} for image #{image_idx}\")\n",
        "        if 'operation' in command.keys() and command['operation'] in valid_operations:\n",
        "          try:\n",
        "            perturbed_img = apply_action(perturbed_img, command)\n",
        "            perturbation_count += 1\n",
        "          except:\n",
        "            print(f\"Error in operation: {command['operation']}\")\n",
        "            print(command)\n",
        "            remove_indices.append(json_list.index(command))\n",
        "        else:\n",
        "          print(f\"Illegal operation or format: {command}\")\n",
        "          remove_indices.append(json_list.index(command))\n",
        "\n",
        "      # Show original and perturbed image\n",
        "\n",
        "      c, s, probabilities = classify_image(perturbed_img, model=model, data=test)\n",
        "      c = str(c).lower()\n",
        "\n",
        "      print(c)\n",
        "      print(true_label)\n",
        "\n",
        "      if c.strip() == true_label.strip():\n",
        "        print(f'Correctly classified perturbed image {image_idx}')\n",
        "        correct += 1\n",
        "      else:\n",
        "        print(f'Classified perturbed image {image_idx} wrong')\n",
        "        misclassified_images.append(perturbed_img)\n",
        "        misclassified_true_images.append(cur_PIL)\n",
        "\n",
        "\n",
        "      similarity_score = image_distance(cur_PIL, perturbed_img, clip_model, clip_preprocess)\n",
        "\n",
        "      one_hot_encoding_of_label = torch.zeros(10, device=device)\n",
        "      one_hot_encoding_of_label[probabilities.argmax()] = 1\n",
        "      loss = loss_func(one_hot_encoding_of_label, probabilities, similarity_score)\n",
        "\n",
        "      json_list = [json_list[i] for i in range(len(json_list)) if i not in remove_indices]\n",
        "      print(f\"Current Loss: {loss}\")\n",
        "      cur_loss.append(loss)\n",
        "      true_labels.append(true_label)\n",
        "      perturbations.append(json.dumps(json_list))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  current_img += 1\n"
      ],
      "metadata": {
        "id": "djcs4FYWFyhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bc2d25-5677-4343-ef28-13fc800b37de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING EPOCH 0 FOR IMAGE #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying perturbations...\n",
            "apply perturbation #0 for image #0\n",
            "apply perturbation #1 for image #0\n",
            "apply perturbation #2 for image #0\n",
            "apply perturbation #3 for image #0\n",
            "apply perturbation #4 for image #0\n",
            "apply perturbation #5 for image #0\n",
            "apply perturbation #6 for image #0\n",
            "cuda\n",
            "('golf ball',)\n",
            "('golf ball',)\n",
            "Correctly classified perturbed image 0\n",
            "Current Loss: tensor([1.3954], device='cuda:0')\n",
            "STARTING EPOCH 1 FOR IMAGE #0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check final accuracy of inferences\n",
        "print(f\"Baseline Accuracy (from fine-tuning): {baseline_accuracy}%\")\n",
        "print(f\"Final Accuracy: {(100 * (correct/(len(test)*3)))}%\")"
      ],
      "metadata": {
        "id": "tj6TjlBJXvzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cur_loss"
      ],
      "metadata": {
        "id": "5KPgcigmv_fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(misclassified_images[2])"
      ],
      "metadata": {
        "id": "hKkDtHzxW2ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(misclassified_true_images[2])"
      ],
      "metadata": {
        "id": "pKUy5O74YjjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "cur_loss_floats = [loss.item() for loss in cur_loss]\n",
        "\n",
        "avg_loss_per_image = [\n",
        "    sum(cur_loss_floats[i:i+3]) / 3 for i in range(0, len(cur_loss_floats), 3)\n",
        "]\n",
        "\n",
        "x = list(range(len(avg_loss_per_image)))\n",
        "fit = np.polyfit(x, avg_loss_per_image, deg=1)\n",
        "fit_line = np.poly1d(fit)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(x, avg_loss_per_image, marker='o', label=\"Avg Loss per Image\")\n",
        "plt.plot(x, fit_line(x), linestyle='--', color='red', label=\"Best Fit Line\")\n",
        "plt.xlabel(\"Image Index\")\n",
        "plt.ylabel(\"Average Score\")\n",
        "plt.title(\"Average Score per Image (with Line of Best Fit)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RsFOyowgNpnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_loss = 0\n",
        "second_loss = 0\n",
        "third_loss = 0\n",
        "for i in range(len(cur_loss)):\n",
        "  if i % 3 == 0:\n",
        "    first_loss += cur_loss[i]/(len(cur_loss)/3)\n",
        "  elif i % 3 == 1:\n",
        "    second_loss += cur_loss[i]/(len(cur_loss)/3)\n",
        "  else:\n",
        "    third_loss += cur_loss[i]/(len(cur_loss)/3)"
      ],
      "metadata": {
        "id": "JmmdSlqhNner"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "total2 = 0\n",
        "\n",
        "for i in range(len(misclassified_images)):\n",
        "  distance = image_distance(misclassified_true_images[i], misclassified_images[i], clip_model, clip_preprocess)\n",
        "  total += distance\n",
        "  #print(distance)\n",
        "  distance2  = image_distance2(np.array(misclassified_true_images[i]), np.array(misclassified_images[i]))\n",
        "  total2 += distance2\n",
        "\n",
        "avg = total / len(misclassified_images)\n",
        "print(len(misclassified_images))\n",
        "print(f\"Average distance between misclassified images: {avg}\")\n",
        "avg2 = total2 / len(misclassified_images)\n",
        "print(f\"Average distance between misclassified images: {avg2}\")"
      ],
      "metadata": {
        "id": "u5qfMTm5NPBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdOtbPWUPGhP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}