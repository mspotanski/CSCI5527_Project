{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trickng ResNet18 Image Classifier Using LLM Suggested Image Perturbations"
      ],
      "metadata": {
        "id": "ZSxEHJXKt73P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWCVNMHLJrmB"
      },
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install missing packages from main implementation\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -q transformers accelerate bitsandbytes torch\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "dedAlOBJMggL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, datasets\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "NZw6LALN1K0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test images\n",
        "# url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "# try: urllib.URLopener().retrieve(url, filename)\n",
        "# except: urllib.request.urlretrieve(url, filename)\n",
        "# img = Image.open(filename)"
      ],
      "metadata": {
        "id": "7197SUYYkuXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with  token\n",
        "hf_token = userdata.get('HF_TOKEN')  # Retrieves token --> Assuming that everyone needs to make their own hugging face account?\n",
        "\n",
        "# Load model with authentication\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_token,\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    ),\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "8VEOUDsLMz5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test that the model and image loading worked\n",
        "def generate_response(prompt, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = llm_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "DgVVyicmGrQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPTING"
      ],
      "metadata": {
        "id": "Jnbvbemi1sz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perturbations_prompt = \"Give locations, size, and type of perturbations to be done to an image. The image size is 1920 by 1080 and the maximum perturbation size is 100 by 100. Give a list of 10 perturbations which specify pixel location, size of the perturbation patch, channel an integer between 0 and 3, and type of perturbation. Respond only in JSON format with no explainations.\"\n",
        "\n",
        "# perturbation_examples = \"\"\"  {\"location\": [20, 30], \"size\": [10, 10], \"channel\": 0, \"type\": \"gaussian_noise\"},\n",
        "#   {\"location\": [50, 60], \"size\": [8, 8], \"channel\": 0, \"type\": \"blur\"},\n",
        "#   {\"location\": [70, 20], \"size\": [6, 6], \"channel\": 1, \"type\": \"occlusion\"},\n",
        "#   {\"location\": [10, 10], \"size\": [9, 9], \"channel\": 2, \"type\": \"brightness_increase\"},\n",
        "#   {\"location\": [80, 40], \"size\": [7, 7], \"channel\": 0, \"type\": \"contrast_decrease\"},\n",
        "#   {\"location\": [30, 70], \"size\": [10, 10], \"channel\": 1, \"type\": \"salt_and_pepper_noise\"},\n",
        "#   {\"location\": [60, 15], \"size\": [5, 5], \"channel\": 2, \"type\": \"motion_blur\"},\n",
        "#   {\"location\": [25, 85], \"size\": [6, 6], \"channel\": 1, \"type\": \"color_shift\"},\n",
        "#   {\"location\": [90, 90], \"size\": [10, 10], \"channel\": 2, \"type\": \"sharpen\"},\n",
        "#   {\"location\": [40, 50], \"size\": [7, 7], \"channel\": 0, \"type\": \"grayscale\"}\n",
        "#   \"\"\"\n",
        "\n",
        "# ranks = [0.9,1,0.2,0.3,0.1,0.5,0,0.2,0.2,0.8]\n",
        "\n"
      ],
      "metadata": {
        "id": "XxcYBxiShWLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perturbations_prompt = \"Give types of perturbations to be done to an image. The image size is 256 by 256.\\\n",
        "Give a list of up to 10 perturbations where operations are one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "Rotate requires an angle parameter.\\\n",
        "adjust_brightness requires a factor parameter.\\\n",
        "blur_patch requires center, radius, and sigma parameters.\\\n",
        "add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "translate requires x_shift and y_shift parameters.\\\n",
        "Respond only in JSON format with no explainations.\"\n",
        "\n",
        "perturbation_examples = \"\"\"\n",
        "    [{\n",
        "            \"operation\": \"rotate\",\n",
        "            \"angle\": 15\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"adjust_brightness\",\n",
        "            \"factor\": 1.5\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"blur_patch\",\n",
        "            \"center\": [80, 60],\n",
        "            \"radius\": 20,\n",
        "            \"sigma\": 5.0\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_stripe_noise\",\n",
        "            \"orientation\": \"horizontal\",\n",
        "            \"stripe_width\": 10,\n",
        "            \"intensity\": 0.1,\n",
        "            \"location\": 0.3\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_patch\",\n",
        "            \"location\": [50, 50],\n",
        "            \"size\": [40, 40],\n",
        "            \"type\": \"noise\"\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_patch\",\n",
        "            \"location\": [120, 120],\n",
        "            \"size\": [30, 30],\n",
        "            \"type\": \"color\",\n",
        "            \"color\": [255, 0, 0]\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"translate\",\n",
        "            \"x_shift\": 20,\n",
        "            \"y_shift\": 30\n",
        "    }]\"\"\""
      ],
      "metadata": {
        "id": "MVNe8F58cbIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original prompt is below:\n",
        "# f\"Give types of perturbations to be done to an image. The image size is {size[0]} by {size[1]}.\\\n",
        "# Give a list of up to 10 perturbations where operations are one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "# Rotate requires an angle parameter.\\\n",
        "# adjust_brightness requires a factor parameter.\\\n",
        "# blur_patch requires center, radius, and sigma parameters.\\\n",
        "# add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "# add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "# translate requires x_shift and y_shift parameters.\\\n",
        "# Respond only in JSON format with no explainations.\""
      ],
      "metadata": {
        "id": "0GRoonFfj8tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_new_perturbations_prompt(prev_pert = perturbation_examples, ranks = [], size = [224,224]):\n",
        "  perturbations_prompt = f\"Give a list of up to 10 image perturbations that can trick ResNet's image classifier into misclassifying the perturbed image while keeping the perturbed image as close to the original one as possible. Strictly follow these guidelines: \\\n",
        "Operations are exclusively one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "The image size is {size[0]} by {size[1]}.\\\n",
        "Rotate requires an angle parameter.\\\n",
        "adjust_brightness requires a factor parameter.\\\n",
        "blur_patch requires center, radius, and sigma parameters.\\\n",
        "add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "translate requires x_shift and y_shift parameters.\\\n",
        "Strictly respond only in JSON format with no explanations.\n",
        "  if ranks == []:\n",
        "    prompt = perturbations_prompt + \" Examples: \" + prev_pert + \"Generate new perturbations, only using JSON \\n\"\n",
        "  else:\n",
        "    prompt = perturbations_prompt + \" Previous responses: \" + prev_pert + \" Scored between 0 and infinite where 0 is better: \" + str(ranks) + \" \\nGenerate new perturbations without explanations, only using JSON \\n\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "qRWRV35MmsiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = generate_new_perturbations_prompt()\n",
        "print(test)"
      ],
      "metadata": {
        "id": "6H1PylBoqawr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Select only the json list from the response\n",
        "def extract_text_between_brackets(text):\n",
        "  text = text.split(\"only using JSON \\n\")[1] # Ignore example json list\n",
        "  text = text.replace(' ', '')\n",
        "  text = text.replace('\\n', '')\n",
        "  text = text.replace('\\\\','')\n",
        "  if len(re.findall(r\"\\[\\{(.*?)\\}\\]\", text, re.DOTALL)) == 0: # If no full JSON exists return empty list\n",
        "    return \"[]\"\n",
        "  return \"[{\"+re.findall(r\"\\[\\{(.*?)\\}\\]\", text, re.DOTALL)[0]+\"}]\""
      ],
      "metadata": {
        "id": "zMfWJmZVtvrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = generate_response(test, 400)\n",
        "# print(extract_text_between_brackets(response))\n",
        "# commands = json.loads(extract_text_between_brackets(response))"
      ],
      "metadata": {
        "id": "3WIN0m5duWxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERTURBATOR"
      ],
      "metadata": {
        "id": "zLuNRYZ4jwWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_image(image, angle):\n",
        "    return image.rotate(angle)\n",
        "\n",
        "def adjust_brightness(image, factor):\n",
        "    enhancer = ImageEnhance.Brightness(image)\n",
        "    return enhancer.enhance(factor)\n",
        "\n",
        "def blur_patch(image, center, radius, sigma):\n",
        "    img_np = np.array(image)\n",
        "    (h, w, _) = img_np.shape\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "    cv2.circle(mask, tuple(center), radius, 255, -1)\n",
        "\n",
        "    blurred = cv2.GaussianBlur(img_np, (0, 0), sigma)\n",
        "\n",
        "    mask = mask[:, :, np.newaxis] / 255.0\n",
        "    output = img_np * (1 - mask) + blurred * mask\n",
        "    output = np.clip(output, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return Image.fromarray(output)\n",
        "\n",
        "def add_stripe_noise(image, orientation, stripe_width, intensity, location=0):\n",
        "    img_np = np.array(image).astype(np.float32) / 255.0\n",
        "    noise = np.random.uniform(-intensity, intensity, img_np.shape)\n",
        "\n",
        "    mask = np.zeros_like(img_np)\n",
        "    H, W, _ = img_np.shape\n",
        "\n",
        "    if orientation == \"horizontal\":\n",
        "        # Horizontal stripe centered at given y-location\n",
        "        y_center = int(location * H)\n",
        "        y_start = max(0, y_center - stripe_width // 2)\n",
        "        y_end = min(H, y_center + stripe_width // 2)\n",
        "        mask[y_start:y_end, :, :] = 1\n",
        "    elif orientation == \"vertical\":\n",
        "        # Vertical stripe centered at given x-location\n",
        "        x_center = int(location * W)\n",
        "        x_start = max(0, x_center - stripe_width // 2)\n",
        "        x_end = min(W, x_center + stripe_width // 2)\n",
        "        mask[:, x_start:x_end, :] = 1\n",
        "\n",
        "    noisy_img = np.clip(img_np + noise * mask, 0, 1)\n",
        "    noisy_img = (noisy_img * 255).astype(np.uint8)\n",
        "    return Image.fromarray(noisy_img)\n",
        "\n",
        "def add_patch(image, location, size, type_=\"noise\", color=None):\n",
        "    img_np = np.array(image)\n",
        "\n",
        "    patch = None\n",
        "    if type_ == \"noise\":\n",
        "        patch = np.random.randint(0, 256, (size[1], size[0], 3), dtype=np.uint8)\n",
        "    elif type_ == \"color\" and color is not None:\n",
        "        patch = np.ones((size[1], size[0], 3), dtype=np.uint8) * np.array(color, dtype=np.uint8)\n",
        "\n",
        "    x, y = location  # Now input is exact (x, y)\n",
        "\n",
        "    # Boundary check\n",
        "    H, W, _ = img_np.shape\n",
        "    x = max(0, min(x, W - size[0]))\n",
        "    y = max(0, min(y, H - size[1]))\n",
        "\n",
        "    img_np[y:y+size[1], x:x+size[0]] = patch\n",
        "\n",
        "    return Image.fromarray(img_np)\n",
        "\n",
        "def translate_image(image, x_shift, y_shift):\n",
        "    img_np = np.array(image)\n",
        "    (h, w) = img_np.shape[:2]\n",
        "\n",
        "    M = np.float32([[1, 0, x_shift], [0, 1, y_shift]])\n",
        "    shifted = cv2.warpAffine(img_np, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
        "\n",
        "    return Image.fromarray(shifted)\n"
      ],
      "metadata": {
        "id": "coVWyHoUjyh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_action(image, action_dict):\n",
        "    operation = action_dict[\"operation\"]\n",
        "\n",
        "    if operation == \"rotate\":\n",
        "        return rotate_image(image, angle=action_dict[\"angle\"])\n",
        "    elif operation == \"adjust_brightness\":\n",
        "        return adjust_brightness(image, factor=action_dict[\"factor\"])\n",
        "    elif operation == \"blur_patch\":\n",
        "        return blur_patch(\n",
        "            image,\n",
        "            center=action_dict[\"center\"],\n",
        "            radius=action_dict[\"radius\"],\n",
        "            sigma=action_dict[\"sigma\"]\n",
        "        )\n",
        "    elif operation == \"add_stripe_noise\":\n",
        "        return add_stripe_noise(\n",
        "            image,\n",
        "            orientation=action_dict[\"orientation\"],\n",
        "            stripe_width=action_dict[\"stripe_width\"],\n",
        "            intensity=action_dict[\"intensity\"],\n",
        "            location=action_dict.get(\"location\", 0)\n",
        "        )\n",
        "    elif operation == \"add_patch\":\n",
        "        return add_patch(\n",
        "            image,\n",
        "            location=action_dict[\"location\"],\n",
        "            size=action_dict[\"size\"],\n",
        "            type_=action_dict.get(\"type\", \"noise\"),\n",
        "            color=action_dict.get(\"color\")\n",
        "        )\n",
        "    elif operation == \"translate\":\n",
        "        return translate_image(\n",
        "            image,\n",
        "            x_shift=action_dict[\"x_shift\"],\n",
        "            y_shift=action_dict[\"y_shift\"]\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown operation: {operation}\")\n"
      ],
      "metadata": {
        "id": "pUqsJ-H-jyfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image, title=\"Image\"):\n",
        "    plt.imshow(np.array(image))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# for command in commands:\n",
        "#   perturbed_img = apply_action(img, command)\n",
        "#   show_image(perturbed_img, title=f\"Perturbation: {command['operation']}\")"
      ],
      "metadata": {
        "id": "ragD8kC1jyc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESNET"
      ],
      "metadata": {
        "id": "3lBQj-a_1q-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "# # or any of these variants\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
        "# resnet_model.eval()"
      ],
      "metadata": {
        "id": "pKYCuqVT1qhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ImageNet labels\n",
        "# !wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
      ],
      "metadata": {
        "id": "l4WPE7jR1zxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image, model=torch.load(\"resnet18_CIFAR10.pth\"),\n",
        "                   data=datasets.CIFAR10(root='.', train=False, download=True)):\n",
        "  # model should be preloaded fine-tuned model from earlier\n",
        "  # data is the CIFAR 10 test dataset\n",
        "  # image is the current image from the test set\n",
        "  # sample execution (requires torchvision)\n",
        "  # this could def be implemented a lot nicer\n",
        "  input_image = image\n",
        "  preprocess = transforms.Compose([\n",
        "      transforms.Resize(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "  # Normalize input image\n",
        "  input_tensor = preprocess(input_image)\n",
        "\n",
        "  # create a mini-batch as expected by the model\n",
        "  input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "  # Make prediction on label\n",
        "  with torch.no_grad():\n",
        "      output = model(input_batch)\n",
        "  # Tensor of shape 10, with confidence scores over CIFAR10's 10 classes\n",
        "  # print(output[0])\n",
        "  # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "  probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "  # print(probabilities)\n",
        "\n",
        "  # Read the categories\n",
        "  # should only pass\n",
        "  categories = data.classes\n",
        "\n",
        "  # Show top categories per image\n",
        "  top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "  # for i in range(top5_prob.size(0)):\n",
        "  #     print(categories[top5_catid[i]], top5_prob[i].item())\n",
        "\n",
        "  # Return top prediction\n",
        "  return categories[top5_catid[0]], top5_prob[0].item(), probabilities"
      ],
      "metadata": {
        "id": "_66lg5XT13wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img, label = test[14]\n",
        "# c, s, probabilities = classify_image(img, model=model, data=test)\n",
        "# c"
      ],
      "metadata": {
        "id": "rhdsRVIY21z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP SIMILARITY"
      ],
      "metadata": {
        "id": "VEhL7MfF-aE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize clip comparison objects\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "tPRWVcXY-bXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_distance(image1, image2):\n",
        "  # model, preprocess = clip.load(\"ViT-B/32\", device=device) # Likely move this line out of function so it doesn't have to keep reloading\n",
        "  cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "  image1_preprocess = clip_preprocess(image1).unsqueeze(0).to(device)\n",
        "  image1_features = clip_model.encode_image( image1_preprocess)\n",
        "\n",
        "  image2_preprocess = clip_preprocess(image2).unsqueeze(0).to(device)\n",
        "  image2_features = clip_model.encode_image( image2_preprocess)\n",
        "\n",
        "  similarity = cos(image1_features[0],image2_features[0]).item()\n",
        "  return 1 - (similarity+1)/2\n",
        "\n",
        "\n",
        "def image_distance2(image1, image2):\n",
        "  if image1.shape != image2.shape:\n",
        "      raise ValueError(\"Images must have the same shape\")\n",
        "  return np.linalg.norm(image1.astype(float) - image2.astype(float))\n"
      ],
      "metadata": {
        "id": "YG_HS-HUALJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity_score = image_distance(img, perturbed_img)\n",
        "# similarity_score"
      ],
      "metadata": {
        "id": "0kAcGkQr_6hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOSS"
      ],
      "metadata": {
        "id": "0NEbxmtvE1C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(label, probabilities, similarity):\n",
        "  c = torch.tensor([0.5], device=device) # Hyperparameter\n",
        "  loss = torch.nn.CrossEntropyLoss()\n",
        "  loss = loss(label, probabilities)\n",
        "  return loss - similarity * c"
      ],
      "metadata": {
        "id": "b4tTsXmbE2Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one_hot_encoding_of_label = torch.zeros(10, device=device)\n",
        "# one_hot_encoding_of_label[probabilities.argmax()] = 1\n",
        "# loss_func(one_hot_encoding_of_label, probabilities, similarity_score)"
      ],
      "metadata": {
        "id": "fgg7s_MIE19a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUDO CODE FOR POSSIBLE EXECUTION CYCLE?"
      ],
      "metadata": {
        "id": "H3-IpBoF5b--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing to get images that resnet classifies correctly\n",
        "\n",
        "# prompt = generate_new_perturbations_prompt() ##First Pass\n",
        "# for epochs\n",
        "#   for image, label in images: Should probably be in batch form...\n",
        "#     response = generate_response(prompt, 400) ##First Pass\n",
        "#     json_list = extract_text_between_brackets(response) ##Done\n",
        "#     new_image = perturbator(image, json_list) ##Done\n",
        "#     _, _, probabilities = classify_image(new_image, model=model, data=test) ##Done\n",
        "#     similarity_score = image_distance(image, new_image) ##Done\n",
        "#     loss = loss_func(label, probabilities, similarity_score) ##Done\n",
        "#     prompt = generate_new_perturbations_prompt(json_list, loss) ##First Pass"
      ],
      "metadata": {
        "id": "nnT9kkli5Tlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trick Pre-trained ResNet18 Image Classifier\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cSZ7kXNR0iuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load flowers dataset and ResNet model\n",
        "# Normalize current dataset to specifics of original ImageNet dataset to stabilize and speed up learning\n",
        "# Reseize images to match previous implementation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                     std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "# Only need test since we are running inferences\n",
        "test = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                           transform=transform)\n",
        "\n",
        "# Load ResNet model\n",
        "# For scalability, we will only use the 18-layer version as its the smallest\n",
        "model = torch.load()\n",
        "model = model.to(device)\n",
        "\n",
        "# Set up data pipelines for train,test, and val datasets\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize clip comparison objects\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "qEIuKYiw0oRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_operations = ['rotate', 'adjust_brightness', 'blur_patch',\n",
        "                    'add_stripe_noise', 'add_patch', 'translate']\n",
        "epochs = 3\n",
        "# Set size to 224 to match dataset transformation and ResNet expectations\n",
        "prompt = generate_new_perturbations_prompt(size=224)\n",
        "\n",
        "# Set model to evaluation mode for inferences\n",
        "model.eval()\n",
        "\n",
        "# Make correct counter for accuracy calculation later\n",
        "correct = 0\n",
        "for i in range(epochs):\n",
        "  print(f\"STARTING EPOCH: {i}\")\n",
        "  for image, label in test_loader:\n",
        "    # store true label for later\n",
        "    true_label = label.lower()\n",
        "\n",
        "    # # Replace these lines with the proper label from dataset\n",
        "    # c, s, probabilities = classify_image(image, model=model, data=test)\n",
        "    # one_hot_encoding_of_label = torch.zeros(10, device=device)\n",
        "    # one_hot_encoding_of_label[probabilities.argmax()] = 1\n",
        "    # label = one_hot_encoding_of_label\n",
        "\n",
        "    response = generate_response(prompt, 400)\n",
        "    json_list = json.loads(extract_text_between_brackets(response))\n",
        "    # perturbed_img = image\n",
        "    loss = float('inf')\n",
        "    if json_list != \"[]\":\n",
        "      remove_indices = []\n",
        "      for command in json_list:\n",
        "        if 'operation' in command.keys() and command['operation'] in valid_operations:\n",
        "          try:\n",
        "            perturbed_img = apply_action(image, command)\n",
        "            # show_image(perturbed_img, title=f\"Perturbation: {command['operation']}\")\n",
        "            # Uncomment above line for seeing the operations occur\n",
        "          except:\n",
        "            print(f\"Error in operation: {command['operation']}\")\n",
        "            print(command)\n",
        "            remove_indices.append(json_list.index(command))\n",
        "        else:\n",
        "          print(f\"Illegal operation or format: {command}\")\n",
        "          remove_indices.append(json_list.index(command))\n",
        "        c, s, probabilities = classify_image(perturbed_img, model=model,\n",
        "                                             data=test)\n",
        "        # c is predicted label -> convert to lowercase for comparison\n",
        "        c = str(c).lower()\n",
        "\n",
        "        # update correct count\n",
        "        if c == true_label:\n",
        "          correct += 1\n",
        "\n",
        "        similarity_score = image_distance(image, perturbed_img)\n",
        "        loss = loss_func(label, probabilities, similarity_score)\n",
        "        # Don't feed error examples back (Might want to instead feed them as \"Bad\" examples)\n",
        "        json_list = [json_list[i] for i in range(len(json_list)) if i not in remove_indices]\n",
        "        print(f\"Loss: {loss}\")\n",
        "        # Generate new prompt\n",
        "        prompt = generate_new_perturbations_prompt(json.dumps(json_list), loss,\n",
        "                                                   perturbed_img.size)\n",
        "\n",
        "# Check final accuracy of inferences\n",
        "print(f\"Baseline Accuracy (from fine-tuning): 93.41%\")\n",
        "print(f\"Final Accuracy: {100 * (correct/len(test_loader))}:.2%\")"
      ],
      "metadata": {
        "id": "djcs4FYWFyhR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}