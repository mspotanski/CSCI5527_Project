{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWCVNMHLJrmB",
        "outputId": "bdc363b1-1bcc-4bd9-9478-86f1c2f08f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install missing packages from main implementation\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -q transformers accelerate bitsandbytes torch\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dedAlOBJMggL",
        "outputId": "af808148-246c-413c-ea2f-6905eb26d5ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ozheavx1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ozheavx1\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=d69ebb75f6bffeb588794d201533f24097b3734ab4fe42a91649abc39dffa8d9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-see0rgtk/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib\n",
        "from google.colab import userdata  # Import Colab secrets\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, datasets\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "NZw6LALN1K0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test images\n",
        "# url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "# try: urllib.URLopener().retrieve(url, filename)\n",
        "# except: urllib.request.urlretrieve(url, filename)\n",
        "# img = Image.open(filename)"
      ],
      "metadata": {
        "id": "7197SUYYkuXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with  token\n",
        "hf_token = userdata.get('HF_TOKEN')  # Retrieves token --> Assuming that everyone needs to make their own hugging face account?\n",
        "\n",
        "# Load model with authentication\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_token,\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    ),\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "8VEOUDsLMz5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test that the model and image loading worked\n",
        "def generate_response(prompt, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = llm_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "DgVVyicmGrQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPTING"
      ],
      "metadata": {
        "id": "Jnbvbemi1sz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perturbations_prompt = \"Give locations, size, and type of perturbations to be done to an image. The image size is 1920 by 1080 and the maximum perturbation size is 100 by 100. Give a list of 10 perturbations which specify pixel location, size of the perturbation patch, channel an integer between 0 and 3, and type of perturbation. Respond only in JSON format with no explainations.\"\n",
        "\n",
        "# perturbation_examples = \"\"\"  {\"location\": [20, 30], \"size\": [10, 10], \"channel\": 0, \"type\": \"gaussian_noise\"},\n",
        "#   {\"location\": [50, 60], \"size\": [8, 8], \"channel\": 0, \"type\": \"blur\"},\n",
        "#   {\"location\": [70, 20], \"size\": [6, 6], \"channel\": 1, \"type\": \"occlusion\"},\n",
        "#   {\"location\": [10, 10], \"size\": [9, 9], \"channel\": 2, \"type\": \"brightness_increase\"},\n",
        "#   {\"location\": [80, 40], \"size\": [7, 7], \"channel\": 0, \"type\": \"contrast_decrease\"},\n",
        "#   {\"location\": [30, 70], \"size\": [10, 10], \"channel\": 1, \"type\": \"salt_and_pepper_noise\"},\n",
        "#   {\"location\": [60, 15], \"size\": [5, 5], \"channel\": 2, \"type\": \"motion_blur\"},\n",
        "#   {\"location\": [25, 85], \"size\": [6, 6], \"channel\": 1, \"type\": \"color_shift\"},\n",
        "#   {\"location\": [90, 90], \"size\": [10, 10], \"channel\": 2, \"type\": \"sharpen\"},\n",
        "#   {\"location\": [40, 50], \"size\": [7, 7], \"channel\": 0, \"type\": \"grayscale\"}\n",
        "#   \"\"\"\n",
        "\n",
        "# ranks = [0.9,1,0.2,0.3,0.1,0.5,0,0.2,0.2,0.8]\n",
        "\n"
      ],
      "metadata": {
        "id": "XxcYBxiShWLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perturbations_prompt = \"Give types of perturbations to be done to an image. The image size is 256 by 256.\\\n",
        "Give a list of up to 10 perturbations where operations are one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "Rotate requires an angle parameter.\\\n",
        "adjust_brightness requires a factor parameter.\\\n",
        "blur_patch requires center, radius, and sigma parameters.\\\n",
        "add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "translate requires x_shift and y_shift parameters.\\\n",
        "Respond only in JSON format with no explainations.\"\n",
        "\n",
        "perturbation_examples = \"\"\"\n",
        "    [{\n",
        "            \"operation\": \"rotate\",\n",
        "            \"angle\": 15\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"adjust_brightness\",\n",
        "            \"factor\": 1.5\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"blur_patch\",\n",
        "            \"center\": [80, 60],\n",
        "            \"radius\": 20,\n",
        "            \"sigma\": 5.0\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_stripe_noise\",\n",
        "            \"orientation\": \"horizontal\",\n",
        "            \"stripe_width\": 10,\n",
        "            \"intensity\": 0.1,\n",
        "            \"location\": 0.3\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_patch\",\n",
        "            \"location\": [50, 50],\n",
        "            \"size\": [40, 40],\n",
        "            \"type\": \"noise\"\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_patch\",\n",
        "            \"location\": [120, 120],\n",
        "            \"size\": [30, 30],\n",
        "            \"type\": \"color\",\n",
        "            \"color\": [255, 0, 0]\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"translate\",\n",
        "            \"x_shift\": 20,\n",
        "            \"y_shift\": 30\n",
        "    }]\"\"\""
      ],
      "metadata": {
        "id": "MVNe8F58cbIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original prompt is below:\n",
        "# f\"Give types of perturbations to be done to an image. The image size is {size[0]} by {size[1]}.\\\n",
        "# Give a list of up to 10 perturbations where operations are one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "# Rotate requires an angle parameter.\\\n",
        "# adjust_brightness requires a factor parameter.\\\n",
        "# blur_patch requires center, radius, and sigma parameters.\\\n",
        "# add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "# add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "# translate requires x_shift and y_shift parameters.\\\n",
        "# Respond only in JSON format with no explainations.\""
      ],
      "metadata": {
        "id": "0GRoonFfj8tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_new_perturbations_prompt(prev_pert = perturbation_examples, ranks = [], size = [224,224]):\n",
        "  perturbations_prompt = f\"Give a list of up to 10 image perturbations that can trick ResNet's image classifier into misclassifying the perturbed image while keeping the perturbed image as close to the original one as possible. Strictly follow these guidelines: \\\n",
        "Operations are exclusively one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "The image size is {size[0]} by {size[1]}.\\\n",
        "Rotate requires an angle parameter.\\\n",
        "adjust_brightness requires a factor parameter.\\\n",
        "blur_patch requires center, radius, and sigma parameters.\\\n",
        "add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "translate requires x_shift and y_shift parameters.\\\n",
        "Strictly respond only in JSON format with no explanations.\n",
        "  if ranks == []:\n",
        "    prompt = perturbations_prompt + \" Examples: \" + prev_pert + \"Generate new perturbations, only using JSON \\n\"\n",
        "  else:\n",
        "    prompt = perturbations_prompt + \" Previous responses: \" + prev_pert + \" Scored between 0 and infinite where 0 is better: \" + str(ranks) + \" \\nGenerate new perturbations without explanations, only using JSON \\n\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "qRWRV35MmsiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = generate_new_perturbations_prompt()\n",
        "print(test)"
      ],
      "metadata": {
        "id": "6H1PylBoqawr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Select only the json list from the response\n",
        "def extract_text_between_brackets(text):\n",
        "  text = text.split(\"only using JSON \\n\")[1] # Ignore example json list\n",
        "  text = text.replace(' ', '')\n",
        "  text = text.replace('\\n', '')\n",
        "  text = text.replace('\\\\','')\n",
        "  if len(re.findall(r\"\\[\\{(.*?)\\}\\]\", text, re.DOTALL)) == 0: # If no full JSON exists return empty list\n",
        "    return \"[]\"\n",
        "  return \"[{\"+re.findall(r\"\\[\\{(.*?)\\}\\]\", text, re.DOTALL)[0]+\"}]\""
      ],
      "metadata": {
        "id": "zMfWJmZVtvrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(test, 400)"
      ],
      "metadata": {
        "id": "3WIN0m5duWxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_text_between_brackets(response))\n",
        "commands = json.loads(extract_text_between_brackets(response))"
      ],
      "metadata": {
        "id": "IS_RAZ22ubYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERTURBATOR"
      ],
      "metadata": {
        "id": "zLuNRYZ4jwWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_image(image, angle):\n",
        "    return image.rotate(angle)\n",
        "\n",
        "def adjust_brightness(image, factor):\n",
        "    enhancer = ImageEnhance.Brightness(image)\n",
        "    return enhancer.enhance(factor)\n",
        "\n",
        "def blur_patch(image, center, radius, sigma):\n",
        "    img_np = np.array(image)\n",
        "    (h, w, _) = img_np.shape\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "    cv2.circle(mask, tuple(center), radius, 255, -1)\n",
        "\n",
        "    blurred = cv2.GaussianBlur(img_np, (0, 0), sigma)\n",
        "\n",
        "    mask = mask[:, :, np.newaxis] / 255.0\n",
        "    output = img_np * (1 - mask) + blurred * mask\n",
        "    output = np.clip(output, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return Image.fromarray(output)\n",
        "\n",
        "def add_stripe_noise(image, orientation, stripe_width, intensity, location=0):\n",
        "    img_np = np.array(image).astype(np.float32) / 255.0\n",
        "    noise = np.random.uniform(-intensity, intensity, img_np.shape)\n",
        "\n",
        "    mask = np.zeros_like(img_np)\n",
        "    H, W, _ = img_np.shape\n",
        "\n",
        "    if orientation == \"horizontal\":\n",
        "        # Horizontal stripe centered at given y-location\n",
        "        y_center = int(location * H)\n",
        "        y_start = max(0, y_center - stripe_width // 2)\n",
        "        y_end = min(H, y_center + stripe_width // 2)\n",
        "        mask[y_start:y_end, :, :] = 1\n",
        "    elif orientation == \"vertical\":\n",
        "        # Vertical stripe centered at given x-location\n",
        "        x_center = int(location * W)\n",
        "        x_start = max(0, x_center - stripe_width // 2)\n",
        "        x_end = min(W, x_center + stripe_width // 2)\n",
        "        mask[:, x_start:x_end, :] = 1\n",
        "\n",
        "    noisy_img = np.clip(img_np + noise * mask, 0, 1)\n",
        "    noisy_img = (noisy_img * 255).astype(np.uint8)\n",
        "    return Image.fromarray(noisy_img)\n",
        "\n",
        "def add_patch(image, location, size, type_=\"noise\", color=None):\n",
        "    img_np = np.array(image)\n",
        "\n",
        "    patch = None\n",
        "    if type_ == \"noise\":\n",
        "        patch = np.random.randint(0, 256, (size[1], size[0], 3), dtype=np.uint8)\n",
        "    elif type_ == \"color\" and color is not None:\n",
        "        patch = np.ones((size[1], size[0], 3), dtype=np.uint8) * np.array(color, dtype=np.uint8)\n",
        "\n",
        "    x, y = location  # Now input is exact (x, y)\n",
        "\n",
        "    # Boundary check\n",
        "    H, W, _ = img_np.shape\n",
        "    x = max(0, min(x, W - size[0]))\n",
        "    y = max(0, min(y, H - size[1]))\n",
        "\n",
        "    img_np[y:y+size[1], x:x+size[0]] = patch\n",
        "\n",
        "    return Image.fromarray(img_np)\n",
        "\n",
        "def translate_image(image, x_shift, y_shift):\n",
        "    img_np = np.array(image)\n",
        "    (h, w) = img_np.shape[:2]\n",
        "\n",
        "    M = np.float32([[1, 0, x_shift], [0, 1, y_shift]])\n",
        "    shifted = cv2.warpAffine(img_np, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
        "\n",
        "    return Image.fromarray(shifted)\n"
      ],
      "metadata": {
        "id": "coVWyHoUjyh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_action(image, action_dict):\n",
        "    operation = action_dict[\"operation\"]\n",
        "\n",
        "    if operation == \"rotate\":\n",
        "        return rotate_image(image, angle=action_dict[\"angle\"])\n",
        "    elif operation == \"adjust_brightness\":\n",
        "        return adjust_brightness(image, factor=action_dict[\"factor\"])\n",
        "    elif operation == \"blur_patch\":\n",
        "        return blur_patch(\n",
        "            image,\n",
        "            center=action_dict[\"center\"],\n",
        "            radius=action_dict[\"radius\"],\n",
        "            sigma=action_dict[\"sigma\"]\n",
        "        )\n",
        "    elif operation == \"add_stripe_noise\":\n",
        "        return add_stripe_noise(\n",
        "            image,\n",
        "            orientation=action_dict[\"orientation\"],\n",
        "            stripe_width=action_dict[\"stripe_width\"],\n",
        "            intensity=action_dict[\"intensity\"],\n",
        "            location=action_dict.get(\"location\", 0)\n",
        "        )\n",
        "    elif operation == \"add_patch\":\n",
        "        return add_patch(\n",
        "            image,\n",
        "            location=action_dict[\"location\"],\n",
        "            size=action_dict[\"size\"],\n",
        "            type_=action_dict.get(\"type\", \"noise\"),\n",
        "            color=action_dict.get(\"color\")\n",
        "        )\n",
        "    elif operation == \"translate\":\n",
        "        return translate_image(\n",
        "            image,\n",
        "            x_shift=action_dict[\"x_shift\"],\n",
        "            y_shift=action_dict[\"y_shift\"]\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown operation: {operation}\")\n"
      ],
      "metadata": {
        "id": "pUqsJ-H-jyfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image, title=\"Image\"):\n",
        "    plt.imshow(np.array(image))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for command in commands:\n",
        "  perturbed_img = apply_action(img, command)\n",
        "  show_image(perturbed_img, title=f\"Perturbation: {command['operation']}\")"
      ],
      "metadata": {
        "id": "ragD8kC1jyc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESNET"
      ],
      "metadata": {
        "id": "3lBQj-a_1q-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "# or any of these variants\n",
        "# resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
        "# resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
        "# resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
        "# resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
        "resnet_model.eval()"
      ],
      "metadata": {
        "id": "pKYCuqVT1qhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ImageNet labels\n",
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
      ],
      "metadata": {
        "id": "l4WPE7jR1zxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image, model=torch.load(\"resnet18_CIFAR10.pth\"),\n",
        "                   data=datasets.CIFAR10(root='.', train=False, download=True)):\n",
        "  # model should be preloaded fine-tuned model from earlier\n",
        "  # data is the CIFAR 10 test dataset\n",
        "  # image is the current image from the test set\n",
        "  # sample execution (requires torchvision)\n",
        "  # this could def be implemented a lot nicer\n",
        "  input_image = image\n",
        "  preprocess = transforms.Compose([\n",
        "      transforms.Resize(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "  # Normalize input image\n",
        "  input_tensor = preprocess(input_image)\n",
        "\n",
        "  # create a mini-batch as expected by the model\n",
        "  input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "  # Make prediction on label\n",
        "  with torch.no_grad():\n",
        "      output = resnet_model(input_batch)\n",
        "  # Tensor of shape 10, with confidence scores over CIFAR10's 10 classes\n",
        "  # print(output[0])\n",
        "  # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "  probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "  # print(probabilities)\n",
        "\n",
        "  # Read the categories\n",
        "  # should only pass\n",
        "  categories = data.classes\n",
        "\n",
        "  # Show top categories per image\n",
        "  top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "  # for i in range(top5_prob.size(0)):\n",
        "  #     print(categories[top5_catid[i]], top5_prob[i].item())\n",
        "\n",
        "  # Return top prediction\n",
        "  return categories[top5_catid[0]], top5_prob[0].item(), probabilities"
      ],
      "metadata": {
        "id": "_66lg5XT13wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img =\n",
        "c, s, probabilities = classify_image(img)"
      ],
      "metadata": {
        "id": "rhdsRVIY21z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "id": "6i-RelPs3Lwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP SIMILARITY"
      ],
      "metadata": {
        "id": "VEhL7MfF-aE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "tPRWVcXY-bXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_distance(image1, image2):\n",
        "  # model, preprocess = clip.load(\"ViT-B/32\", device=device) # Likely move this line out of function so it doesn't have to keep reloading\n",
        "  cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "  image1_preprocess = clip_preprocess(image1).unsqueeze(0).to(device)\n",
        "  image1_features = clip_model.encode_image( image1_preprocess)\n",
        "\n",
        "  image2_preprocess = clip_preprocess(image2).unsqueeze(0).to(device)\n",
        "  image2_features = clip_model.encode_image( image2_preprocess)\n",
        "\n",
        "  similarity = cos(image1_features[0],image2_features[0]).item()\n",
        "  return 1 - (similarity+1)/2\n",
        "\n",
        "\n",
        "def image_distance2(image1, image2):\n",
        "  if image1.shape != image2.shape:\n",
        "      raise ValueError(\"Images must have the same shape\")\n",
        "  return np.linalg.norm(image1.astype(float) - image2.astype(float))\n"
      ],
      "metadata": {
        "id": "YG_HS-HUALJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_score = image_distance(img, perturbed_img)\n",
        "similarity_score"
      ],
      "metadata": {
        "id": "0kAcGkQr_6hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOSS"
      ],
      "metadata": {
        "id": "0NEbxmtvE1C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(label, probabilities, similarity):\n",
        "  c = torch.tensor([0.5], device=device) # Hyperparameter\n",
        "  loss = torch.nn.CrossEntropyLoss()\n",
        "  loss = loss(label, probabilities)\n",
        "  return loss - similarity * c"
      ],
      "metadata": {
        "id": "b4tTsXmbE2Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding_of_label = torch.zeros(10, device=device)\n",
        "one_hot_encoding_of_label[probabilities.argmax()] = 1\n",
        "loss_func(one_hot_encoding_of_label, probabilities, similarity_score)"
      ],
      "metadata": {
        "id": "fgg7s_MIE19a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUDO CODE FOR POSSIBLE EXECUTION CYCLE?"
      ],
      "metadata": {
        "id": "H3-IpBoF5b--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing to get images that resnet classifies correctly\n",
        "\n",
        "# prompt = generate_new_perturbations_prompt() ##First Pass\n",
        "# for epochs\n",
        "#   for image, label in images: Should probably be in batch form...\n",
        "#     response = generate_response(prompt, 400) ##First Pass\n",
        "#     json_list = extract_text_between_brackets(response) ##Done\n",
        "#     new_image = perturbator(image, json_list) ##Done\n",
        "#     _, _, probabilities = classify_image(new_image) ##Done\n",
        "#     similarity_score = image_distance(image, new_image) ##Done\n",
        "#     loss = loss_func(label, probabilities, similarity_score) ##Done\n",
        "#     prompt = generate_new_perturbations_prompt(json_list, loss) ##First Pass"
      ],
      "metadata": {
        "id": "nnT9kkli5Tlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune Pre-trained ResNet18 Image Classifier"
      ],
      "metadata": {
        "id": "cSZ7kXNR0iuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load flowers dataset and ResNet model\n",
        "# Normalize current dataset to specifics of original ImageNet dataset to stabilize and speed up learning\n",
        "# Reseize images to match previous implementation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                     std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "# Only need test since we are running inferences\n",
        "test = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                           transform=transform)\n",
        "\n",
        "# Load ResNet model\n",
        "# For scalability, we will only use the 18-layer version as its the smallest\n",
        "model = torch.load()\n",
        "model = model.to(device)\n",
        "\n",
        "# Set up data pipelines for train,test, and val datasets\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "qEIuKYiw0oRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_operations = ['rotate', 'adjust_brightness', 'blur_patch',\n",
        "                    'add_stripe_noise', 'add_patch', 'translate']\n",
        "epochs = 3\n",
        "# Set size to 224 to match dataset transformation and ResNet expectations\n",
        "prompt = generate_new_perturbations_prompt(size=224)\n",
        "\n",
        "# Set model to evaluation mode for inferences\n",
        "model.eval()\n",
        "\n",
        "# Make correct counter for accuracy calculation later\n",
        "correct = 0\n",
        "for i in range(epochs):\n",
        "  print(f\"STARTING EPOCH: {i}\")\n",
        "  for image, label in test_loader:\n",
        "    # store true label for later\n",
        "    true_label = label\n",
        "\n",
        "    # # Replace these lines with the proper label from dataset\n",
        "    # c, s, probabilities = classify_image(image)\n",
        "    # one_hot_encoding_of_label = torch.zeros(10, device=device)\n",
        "    # one_hot_encoding_of_label[probabilities.argmax()] = 1\n",
        "    # label = one_hot_encoding_of_label\n",
        "\n",
        "    response = generate_response(prompt, 400)\n",
        "    json_list = json.loads(extract_text_between_brackets(response))\n",
        "    # perturbed_img = image\n",
        "    loss = float('inf')\n",
        "    if json_list != \"[]\":\n",
        "      remove_indices = []\n",
        "      for command in json_list:\n",
        "        if 'operation' in command.keys() and command['operation'] in valid_operations:\n",
        "          try:\n",
        "            perturbed_img = apply_action(image, command)\n",
        "            # show_image(perturbed_img, title=f\"Perturbation: {command['operation']}\")\n",
        "            # Uncomment above line for seeing the operations occur\n",
        "          except:\n",
        "            print(f\"Error in operation: {command['operation']}\")\n",
        "            print(command)\n",
        "            remove_indices.append(json_list.index(command))\n",
        "        else:\n",
        "          print(f\"Illegal operation or format: {command}\")\n",
        "          remove_indices.append(json_list.index(command))\n",
        "        c, s, probabilities = classify_image(perturbed_img)\n",
        "        similarity_score = image_distance(img, perturbed_img)\n",
        "        loss = loss_func(label, probabilities, similarity_score)\n",
        "        # Don't feed error examples back (Might want to instead feed them as \"Bad\" examples)\n",
        "        json_list = [json_list[i] for i in range(len(json_list)) if i not in remove_indices]\n",
        "        print(f\"Loss: {loss}\")\n",
        "        # Generate new prompt\n",
        "        prompt = generate_new_perturbations_prompt(json.dumps(json_list), loss,\n",
        "                                                   perturbed_image.size)"
      ],
      "metadata": {
        "id": "djcs4FYWFyhR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}