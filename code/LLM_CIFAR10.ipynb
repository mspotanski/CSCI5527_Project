{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "390fde4a2c814e80965f611695b82eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c8965e68020461eb25b33d24f2d2ed0",
              "IPY_MODEL_4061dd942fb5430e96c5dc0aad2cd060",
              "IPY_MODEL_18d7e9e868e24b1aa57f935c56976d4d"
            ],
            "layout": "IPY_MODEL_4550a51334a840f3a2c6bdbab7e4d021"
          }
        },
        "1c8965e68020461eb25b33d24f2d2ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cb8fdbe96b14a069004fb8bdde5ea44",
            "placeholder": "​",
            "style": "IPY_MODEL_d55ff37706fa4c33843ee19a13dedbf6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4061dd942fb5430e96c5dc0aad2cd060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b705a49de7b4837a25d5142598595dd",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_741bb79bf5384438af5202d3db8c1233",
            "value": 4
          }
        },
        "18d7e9e868e24b1aa57f935c56976d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c18186770f14e7090fba7bff648fdb4",
            "placeholder": "​",
            "style": "IPY_MODEL_6a755e39756340e8aaeb804c2121da27",
            "value": " 4/4 [00:18&lt;00:00,  3.88s/it]"
          }
        },
        "4550a51334a840f3a2c6bdbab7e4d021": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb8fdbe96b14a069004fb8bdde5ea44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d55ff37706fa4c33843ee19a13dedbf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b705a49de7b4837a25d5142598595dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "741bb79bf5384438af5202d3db8c1233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c18186770f14e7090fba7bff648fdb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a755e39756340e8aaeb804c2121da27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trickng ResNet18 Image Classifier Using LLM Suggested Image Perturbations"
      ],
      "metadata": {
        "id": "ZSxEHJXKt73P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWCVNMHLJrmB",
        "outputId": "7b9c5125-d185-4b53-ef8a-0009c0a059e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install missing packages from main implementation\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -q transformers accelerate bitsandbytes torch\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dedAlOBJMggL",
        "outputId": "2407a08d-cd8b-4b9c-e05a-4ea438cbff1c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-jcu4gsz6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-jcu4gsz6\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, datasets\n",
        "import clip\n"
      ],
      "metadata": {
        "id": "NZw6LALN1K0p"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEaGvvpWxIp6",
        "outputId": "1d47812e-995b-4ead-90fc-57753de18ab2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test images\n",
        "# url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "# try: urllib.URLopener().retrieve(url, filename)\n",
        "# except: urllib.request.urlretrieve(url, filename)\n",
        "# img = Image.open(filename)"
      ],
      "metadata": {
        "id": "7197SUYYkuXM"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with token\n",
        "# Retrieves token: Assumes that everyone needs their own hugging face account and add token to colab env\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Load model with authentication\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_token,\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    ),\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "390fde4a2c814e80965f611695b82eb9",
            "1c8965e68020461eb25b33d24f2d2ed0",
            "4061dd942fb5430e96c5dc0aad2cd060",
            "18d7e9e868e24b1aa57f935c56976d4d",
            "4550a51334a840f3a2c6bdbab7e4d021",
            "4cb8fdbe96b14a069004fb8bdde5ea44",
            "d55ff37706fa4c33843ee19a13dedbf6",
            "9b705a49de7b4837a25d5142598595dd",
            "741bb79bf5384438af5202d3db8c1233",
            "7c18186770f14e7090fba7bff648fdb4",
            "6a755e39756340e8aaeb804c2121da27"
          ]
        },
        "id": "8VEOUDsLMz5U",
        "outputId": "4f3487d7-7386-4319-8090-6f3cde51e5a7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "390fde4a2c814e80965f611695b82eb9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test that the model and image loading worked\n",
        "def generate_response(prompt, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = llm_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "DgVVyicmGrQX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPTING"
      ],
      "metadata": {
        "id": "Jnbvbemi1sz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perturbations_prompt = \"Give locations, size, and type of perturbations to be done to an image. The image size is 1920 by 1080 and the maximum perturbation size is 100 by 100. Give a list of 10 perturbations which specify pixel location, size of the perturbation patch, channel an integer between 0 and 3, and type of perturbation. Respond only in JSON format with no explainations.\"\n",
        "\n",
        "# perturbation_examples = \"\"\"  {\"location\": [20, 30], \"size\": [10, 10], \"channel\": 0, \"type\": \"gaussian_noise\"},\n",
        "#   {\"location\": [50, 60], \"size\": [8, 8], \"channel\": 0, \"type\": \"blur\"},\n",
        "#   {\"location\": [70, 20], \"size\": [6, 6], \"channel\": 1, \"type\": \"occlusion\"},\n",
        "#   {\"location\": [10, 10], \"size\": [9, 9], \"channel\": 2, \"type\": \"brightness_increase\"},\n",
        "#   {\"location\": [80, 40], \"size\": [7, 7], \"channel\": 0, \"type\": \"contrast_decrease\"},\n",
        "#   {\"location\": [30, 70], \"size\": [10, 10], \"channel\": 1, \"type\": \"salt_and_pepper_noise\"},\n",
        "#   {\"location\": [60, 15], \"size\": [5, 5], \"channel\": 2, \"type\": \"motion_blur\"},\n",
        "#   {\"location\": [25, 85], \"size\": [6, 6], \"channel\": 1, \"type\": \"color_shift\"},\n",
        "#   {\"location\": [90, 90], \"size\": [10, 10], \"channel\": 2, \"type\": \"sharpen\"},\n",
        "#   {\"location\": [40, 50], \"size\": [7, 7], \"channel\": 0, \"type\": \"grayscale\"}\n",
        "#   \"\"\"\n",
        "\n",
        "# ranks = [0.9,1,0.2,0.3,0.1,0.5,0,0.2,0.2,0.8]\n",
        "\n"
      ],
      "metadata": {
        "id": "XxcYBxiShWLm"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perturbations_prompt = \"Give types of perturbations to be done to an image. The image size is 256 by 256.\\\n",
        "Give a list of up to 10 perturbations where operations are one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "Rotate requires an angle parameter.\\\n",
        "adjust_brightness requires a factor parameter.\\\n",
        "blur_patch requires center, radius, and sigma parameters.\\\n",
        "add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "translate requires x_shift and y_shift parameters.\\\n",
        "Respond only in JSON format with no explainations.\"\n",
        "\n",
        "perturbation_examples = \"\"\"\n",
        "    [{\n",
        "            \"operation\": \"rotate\",\n",
        "            \"angle\": 15\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"adjust_brightness\",\n",
        "            \"factor\": 1.5\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"blur_patch\",\n",
        "            \"center\": [80, 60],\n",
        "            \"radius\": 20,\n",
        "            \"sigma\": 5.0\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_stripe_noise\",\n",
        "            \"orientation\": \"horizontal\",\n",
        "            \"stripe_width\": 10,\n",
        "            \"intensity\": 0.1,\n",
        "            \"location\": 0.3\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_patch\",\n",
        "            \"location\": [50, 50],\n",
        "            \"size\": [40, 40],\n",
        "            \"type\": \"noise\"\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"add_patch\",\n",
        "            \"location\": [120, 120],\n",
        "            \"size\": [30, 30],\n",
        "            \"type\": \"color\",\n",
        "            \"color\": [255, 0, 0]\n",
        "    },\n",
        "    {\n",
        "            \"operation\": \"translate\",\n",
        "            \"x_shift\": 20,\n",
        "            \"y_shift\": 30\n",
        "    }]\"\"\""
      ],
      "metadata": {
        "id": "MVNe8F58cbIM"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original prompt is below:\n",
        "# f\"Give types of perturbations to be done to an image. The image size is {size[0]} by {size[1]}.\\\n",
        "# Give a list of up to 10 perturbations where operations are one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "# Rotate requires an angle parameter.\\\n",
        "# adjust_brightness requires a factor parameter.\\\n",
        "# blur_patch requires center, radius, and sigma parameters.\\\n",
        "# add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "# add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "# translate requires x_shift and y_shift parameters.\\\n",
        "# Respond only in JSON format with no explainations.\""
      ],
      "metadata": {
        "id": "0GRoonFfj8tN"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_new_perturbations_prompt(prev_pert = perturbation_examples, ranks = [], size = [224,224]):\n",
        "  init_prompt = f\"Give a list of up to 10 image perturbations that can trick ResNet's image classifier into misclassifying the perturbed image while keeping the perturbed image as close to the original one as possible. Strictly follow these guidelines:\"\n",
        "  base_prompt = f\"\\\n",
        "Operations are exclusively one of these: [rotate, adjust_brightness, blur_patch, add_stripe_noise, add_patch, translate].\\\n",
        "The image size is {size[0]} by {size[1]}.\\\n",
        "Rotate requires an angle parameter.\\\n",
        "adjust_brightness requires a factor parameter.\\\n",
        "blur_patch requires center, radius, and sigma parameters.\\\n",
        "add_stripe_noise requires orientation, stripe_width, intensity, and location.\\\n",
        "add_patch requires location, size, and type parameters with an option color parameter.\\\n",
        "translate requires x_shift and y_shift parameters.\\\n",
        "Strictly respond only in JSON format with no explanations.\"\n",
        "\n",
        "  perturbations_prompt = f\"{init_prompt}{base_prompt}\"\n",
        "  if ranks == []:\n",
        "    prompt = init_prompt + \"\\Examples: \" + prev_pert + \"Generate new perturbations strictly based on these rules:\" + base_prompt\n",
        "  else:\n",
        "    prompt = \"Previous perturbations were: \" + prev_pert + \" and had a loss of: \" + str(ranks) + \".\\nStricly follow these rules to give a list of up to 10 perturbations to increase that loss value:\\n\" + base_prompt\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "qRWRV35MmsiH"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test = generate_new_perturbations_prompt()\n",
        "# print(test)"
      ],
      "metadata": {
        "id": "6H1PylBoqawr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Select only the json list from the response\n",
        "def extract_text_between_brackets(text):\n",
        "  text = text.split(\"Strictly respond only in JSON format with no explanations.\")[1] # Ignore example json list\n",
        "  text = text.replace(' ', '')\n",
        "  text = text.replace('\\n', '')\n",
        "  text = text.replace('\\\\','')\n",
        "  if len(re.findall(r\"\\[\\{(.*?)\\}\\]\", text, re.DOTALL)) == 0: # If no full JSON exists return empty list\n",
        "    return \"[]\"\n",
        "  return \"[{\"+re.findall(r\"\\[\\{(.*?)\\}\\]\", text, re.DOTALL)[0]+\"}]\""
      ],
      "metadata": {
        "id": "zMfWJmZVtvrK"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = generate_response(test, 400)\n",
        "# print(extract_text_between_brackets(response))\n",
        "# commands = json.loads(extract_text_between_brackets(response))"
      ],
      "metadata": {
        "id": "3WIN0m5duWxx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERTURBATOR"
      ],
      "metadata": {
        "id": "zLuNRYZ4jwWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_image(image, angle):\n",
        "    return image.rotate(angle)\n",
        "\n",
        "def adjust_brightness(image, factor):\n",
        "    enhancer = ImageEnhance.Brightness(image)\n",
        "    return enhancer.enhance(factor)\n",
        "\n",
        "def blur_patch(image, center, radius, sigma):\n",
        "    img_np = np.array(image)\n",
        "    (h, w, _) = img_np.shape\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "    cv2.circle(mask, tuple(center), radius, 255, -1)\n",
        "\n",
        "    blurred = cv2.GaussianBlur(img_np, (0, 0), sigma)\n",
        "\n",
        "    mask = mask[:, :, np.newaxis] / 255.0\n",
        "    output = img_np * (1 - mask) + blurred * mask\n",
        "    output = np.clip(output, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return Image.fromarray(output)\n",
        "\n",
        "def add_stripe_noise(image, orientation, stripe_width, intensity, location=0):\n",
        "    img_np = np.array(image).astype(np.float32) / 255.0\n",
        "    noise = np.random.uniform(-intensity, intensity, img_np.shape)\n",
        "\n",
        "    mask = np.zeros_like(img_np)\n",
        "    H, W, _ = img_np.shape\n",
        "\n",
        "    if orientation == \"horizontal\":\n",
        "        # Horizontal stripe centered at given y-location\n",
        "        y_center = int(location * H)\n",
        "        y_start = max(0, y_center - stripe_width // 2)\n",
        "        y_end = min(H, y_center + stripe_width // 2)\n",
        "        mask[y_start:y_end, :, :] = 1\n",
        "    elif orientation == \"vertical\":\n",
        "        # Vertical stripe centered at given x-location\n",
        "        x_center = int(location * W)\n",
        "        x_start = max(0, x_center - stripe_width // 2)\n",
        "        x_end = min(W, x_center + stripe_width // 2)\n",
        "        mask[:, x_start:x_end, :] = 1\n",
        "\n",
        "    noisy_img = np.clip(img_np + noise * mask, 0, 1)\n",
        "    noisy_img = (noisy_img * 255).astype(np.uint8)\n",
        "    return Image.fromarray(noisy_img)\n",
        "\n",
        "def add_patch(image, location, size, type_=\"noise\", color=None):\n",
        "    img_np = np.array(image)\n",
        "\n",
        "    patch = None\n",
        "    if type_ == \"noise\":\n",
        "        patch = np.random.randint(0, 256, (size[1], size[0], 3), dtype=np.uint8)\n",
        "    elif type_ == \"color\" and color is not None:\n",
        "        patch = np.ones((size[1], size[0], 3), dtype=np.uint8) * np.array(color, dtype=np.uint8)\n",
        "\n",
        "    x, y = location  # Now input is exact (x, y)\n",
        "\n",
        "    # Boundary check\n",
        "    H, W, _ = img_np.shape\n",
        "    x = max(0, min(x, W - size[0]))\n",
        "    y = max(0, min(y, H - size[1]))\n",
        "\n",
        "    img_np[y:y+size[1], x:x+size[0]] = patch\n",
        "\n",
        "    return Image.fromarray(img_np)\n",
        "\n",
        "def translate_image(image, x_shift, y_shift):\n",
        "    img_np = np.array(image)\n",
        "    (h, w) = img_np.shape[:2]\n",
        "\n",
        "    M = np.float32([[1, 0, x_shift], [0, 1, y_shift]])\n",
        "    shifted = cv2.warpAffine(img_np, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
        "\n",
        "    return Image.fromarray(shifted)\n"
      ],
      "metadata": {
        "id": "coVWyHoUjyh8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_action(image, action_dict):\n",
        "    operation = action_dict[\"operation\"]\n",
        "\n",
        "    if operation == \"rotate\":\n",
        "        return rotate_image(image, angle=action_dict[\"angle\"])\n",
        "    elif operation == \"adjust_brightness\":\n",
        "        return adjust_brightness(image, factor=action_dict[\"factor\"])\n",
        "    elif operation == \"blur_patch\":\n",
        "        return blur_patch(\n",
        "            image,\n",
        "            center=action_dict[\"center\"],\n",
        "            radius=action_dict[\"radius\"],\n",
        "            sigma=action_dict[\"sigma\"]\n",
        "        )\n",
        "    elif operation == \"add_stripe_noise\":\n",
        "        return add_stripe_noise(\n",
        "            image,\n",
        "            orientation=action_dict[\"orientation\"],\n",
        "            stripe_width=action_dict[\"stripe_width\"],\n",
        "            intensity=action_dict[\"intensity\"],\n",
        "            location=action_dict.get(\"location\", 0)\n",
        "        )\n",
        "    elif operation == \"add_patch\":\n",
        "        return add_patch(\n",
        "            image,\n",
        "            location=action_dict[\"location\"],\n",
        "            size=action_dict[\"size\"],\n",
        "            type_=action_dict.get(\"type\", \"noise\"),\n",
        "            color=action_dict.get(\"color\")\n",
        "        )\n",
        "    elif operation == \"translate\":\n",
        "        return translate_image(\n",
        "            image,\n",
        "            x_shift=action_dict[\"x_shift\"],\n",
        "            y_shift=action_dict[\"y_shift\"]\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown operation: {operation}\")\n"
      ],
      "metadata": {
        "id": "pUqsJ-H-jyfH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image, title=\"Image\"):\n",
        "    plt.imshow(np.array(image))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# for command in commands:\n",
        "#   perturbed_img = apply_action(img, command)\n",
        "#   show_image(perturbed_img, title=f\"Perturbation: {command['operation']}\")"
      ],
      "metadata": {
        "id": "ragD8kC1jyc_"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESNET"
      ],
      "metadata": {
        "id": "3lBQj-a_1q-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "# # or any of these variants\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
        "# # resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
        "# resnet_model.eval()"
      ],
      "metadata": {
        "id": "pKYCuqVT1qhI"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ImageNet labels\n",
        "# !wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
      ],
      "metadata": {
        "id": "l4WPE7jR1zxe"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image, model=torch.load(\"resnet18_CIFAR10.pth\"),\n",
        "                   data=datasets.CIFAR10(root='.', train=False, download=True)):\n",
        "  # model should be preloaded fine-tuned model from earlier\n",
        "  # data is the CIFAR 10 test dataset\n",
        "  # image is the current image from the test set\n",
        "  # sample execution (requires torchvision)\n",
        "  # this could def be implemented a lot nicer\n",
        "  input_image = image\n",
        "\n",
        "  preprocess = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Resize(224),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "  # Normalize input image\n",
        "  input_tensor = preprocess(input_image)\n",
        "\n",
        "  # create a mini-batch as expected by the model\n",
        "  input_batch = input_tensor.unsqueeze(0)\n",
        "  input_batch = input_batch.to(device)\n",
        "  print(device)\n",
        "\n",
        "  # Make prediction on label\n",
        "  with torch.no_grad():\n",
        "      output = model(input_batch)\n",
        "  # Tensor of shape 10, with confidence scores over CIFAR10's 10 classes\n",
        "  # print(output[0])\n",
        "  # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "  probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "  # print(probabilities)\n",
        "\n",
        "  # Read the categories\n",
        "  categories = data.classes\n",
        "\n",
        "  # Show top category per image\n",
        "  top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "  # for i in range(top5_prob.size(0)):\n",
        "  #     print(categories[top5_catid[i]], top5_prob[i].item())\n",
        "\n",
        "  # Return top prediction\n",
        "  return categories[top5_catid[0]], top5_prob[0].item(), probabilities"
      ],
      "metadata": {
        "id": "_66lg5XT13wW"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img, label = test[14]\n",
        "# c, s, probabilities = classify_image(img, model=model, data=test)\n",
        "# c"
      ],
      "metadata": {
        "id": "rhdsRVIY21z5"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP SIMILARITY"
      ],
      "metadata": {
        "id": "VEhL7MfF-aE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize clip comparison objects\n",
        "# clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "tPRWVcXY-bXO"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_distance(image1, image2, clip_model, clip_preprocess):\n",
        "  clip_model.to(device)\n",
        "  cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "  image1_preprocess = clip_preprocess(image1).unsqueeze(0).to(device)\n",
        "  image1_features = clip_model.encode_image( image1_preprocess)\n",
        "\n",
        "  image2_preprocess = clip_preprocess(image2).unsqueeze(0).to(device)\n",
        "  image2_features = clip_model.encode_image( image2_preprocess)\n",
        "\n",
        "  similarity = cos(image1_features[0],image2_features[0]).item()\n",
        "  return 1 - (similarity+1)/2\n",
        "\n",
        "\n",
        "def image_distance2(image1, image2):\n",
        "  if image1.shape != image2.shape:\n",
        "      raise ValueError(\"Images must have the same shape\")\n",
        "  return np.linalg.norm(image1.astype(float) - image2.astype(float))\n"
      ],
      "metadata": {
        "id": "YG_HS-HUALJS"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity_score = image_distance(img, perturbed_img, clip_model, clip_preprocess)\n",
        "# similarity_score"
      ],
      "metadata": {
        "id": "0kAcGkQr_6hj"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOSS"
      ],
      "metadata": {
        "id": "0NEbxmtvE1C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(label, probabilities, similarity):\n",
        "  c = torch.tensor([0.5], device=device) # Hyperparameter\n",
        "  loss = torch.nn.CrossEntropyLoss()\n",
        "  loss = loss(label, probabilities)\n",
        "  return loss - similarity * c"
      ],
      "metadata": {
        "id": "b4tTsXmbE2Bl"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one_hot_encoding_of_label = torch.zeros(10, device=device)\n",
        "# one_hot_encoding_of_label[probabilities.argmax()] = 1\n",
        "# loss_func(one_hot_encoding_of_label, probabilities, similarity_score)"
      ],
      "metadata": {
        "id": "fgg7s_MIE19a"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUDO CODE FOR POSSIBLE EXECUTION CYCLE?"
      ],
      "metadata": {
        "id": "H3-IpBoF5b--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing to get images that resnet classifies correctly\n",
        "\n",
        "# prompt = generate_new_perturbations_prompt() ##First Pass\n",
        "# for epochs\n",
        "#   for image, label in images: Should probably be in batch form...\n",
        "#     response = generate_response(prompt, 400) ##First Pass\n",
        "#     json_list = extract_text_between_brackets(response) ##Done\n",
        "#     new_image = perturbator(image, json_list) ##Done\n",
        "#     _, _, probabilities = classify_image(new_image, model=model, data=test) ##Done\n",
        "#     similarity_score = image_distance(image, new_image, clip_model, clip_preprocess) ##Done\n",
        "#     loss = loss_func(label, probabilities, similarity_score) ##Done\n",
        "#     prompt = generate_new_perturbations_prompt(json_list, loss) ##First Pass"
      ],
      "metadata": {
        "id": "nnT9kkli5Tlf"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trick Pre-trained ResNet18 Image Classifier\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cSZ7kXNR0iuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load flowers dataset and ResNet model\n",
        "# Normalize current dataset to specifics of original ImageNet dataset to stabilize and speed up learning\n",
        "# Reseize images to match previous implementation\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Resize((224,224)),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                      std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# Load dataset\n",
        "# Only need test since we are running inferences\n",
        "test = datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "# Randomly sample from test set for main dataset\n",
        "# We don't need the entire dataset to show we can tank model performance\n",
        "# test = torch.utils.data.Subset(test, np.random.choice(len(test), 5,\n",
        "#                                                       replace=False))\n",
        "\n",
        "# Load ResNet model\n",
        "# For scalability, we will only use the 18-layer version as its the smallest\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "# Replace the final fully connected layer to output 10 classes\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 10)\n",
        "\n",
        "# Load pretrained state dictionary to model\n",
        "model.load_state_dict(torch.load('/content/resnet18_CIFAR10.pth'))\n",
        "\n",
        "# Map model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Initialize clip comparison objects\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "qEIuKYiw0oRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe6571a-51aa-4826-85b8-8494ec16a453"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_operations = ['rotate', 'adjust_brightness', 'blur_patch',\n",
        "                    'add_stripe_noise', 'add_patch', 'translate']\n",
        "epochs = 1\n",
        "# Set size to 224 to match dataset transformation and ResNet expectations\n",
        "# pass size as list to match prompt implementation\n",
        "prompt = generate_new_perturbations_prompt(size=[224,224])\n",
        "\n",
        "# Set model to evaluation mode for inferences\n",
        "model.eval()\n",
        "\n",
        "# Make counter variables for accuracy calculation later\n",
        "correct = 0\n",
        "current_img = 0\n",
        "\n",
        "misclassified_images = []\n",
        "misclassified_true_images = []\n",
        "perturbations = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  print(f\"STARTING EPOCH: {i}\")\n",
        "  for image, label in test:\n",
        "\n",
        "    if current_img == 5:\n",
        "      break\n",
        "    # store true label for later\n",
        "    true_label = str(test.classes[label]).lower()\n",
        "\n",
        "    # Get PIL image of current image\n",
        "    cur_PIL = image\n",
        "\n",
        "    # Classify image and obtain predicted labels, probabilities\n",
        "    # Convert PIL image back to tensor\n",
        "    # # Replace these lines with the proper label from dataset\n",
        "    # c, s, probabilities = classify_image(image, model=model, data=test)\n",
        "    # one_hot_encoding_of_label = torch.zeros(10, device=device)\n",
        "    # one_hot_encoding_of_label[probabilities.argmax()] = 1\n",
        "    # label = one_hot_encoding_of_label\n",
        "    print(f\"Obtaining perturbations for image #{current_img}\")\n",
        "    response = generate_response(prompt, 400)\n",
        "    json_list = json.loads(extract_text_between_brackets(response))\n",
        "    loss = float('inf')\n",
        "    # initialize perturbed_img object to use outside of try block\n",
        "    perturbed_img = cur_PIL\n",
        "\n",
        "    if json_list != \"[]\":\n",
        "      remove_indices = []\n",
        "      print(\"Applying perturbations...\")\n",
        "      # initialize perturbed_img object to use outside of try block\n",
        "      # perturbed_img = transforms.functional.to_pil_image(image)\n",
        "      perturbation_count = 0\n",
        "      for command in json_list:\n",
        "        print(f\"apply perturbation #{perturbation_count} for image #{current_img}\")\n",
        "        if 'operation' in command.keys() and command['operation'] in valid_operations:\n",
        "          try:\n",
        "            perturbed_img = apply_action(perturbed_img, command)\n",
        "            perturbation_count += 1\n",
        "\n",
        "            # show_image(perturbed_img, title=f\"Perturbation: {command['operation']}\")\n",
        "            # Uncomment above line for seeing the operations occur\n",
        "          except:\n",
        "            print(f\"Error in operation: {command['operation']}\")\n",
        "            print(command)\n",
        "            remove_indices.append(json_list.index(command))\n",
        "        else:\n",
        "          print(f\"Illegal operation or format: {command}\")\n",
        "          remove_indices.append(json_list.index(command))\n",
        "\n",
        "      # Classify image and obtain predicted labels, probabilities\n",
        "      # Convert perturbed PIL image back to tensor\n",
        "      # perturbed_img_tens = transform(perturbed_img)\n",
        "      # perturbed_img_tens = perturbed_img_tens.to(device)\n",
        "      c, s, probabilities = classify_image(perturbed_img, model=model,\n",
        "                                            data=test)\n",
        "      # c is predicted label -> convert to lowercase for comparison\n",
        "      c = str(c).lower()\n",
        "\n",
        "      # update correct count\n",
        "      if c == true_label:\n",
        "        print(f'Correctly classified perturbed image {current_img}')\n",
        "        correct += 1\n",
        "      else:\n",
        "        print(f'Classified perturbed image {current_img} wrong')\n",
        "        # Save misclassified images and indices for later Data Analysis\n",
        "        misclassified_images.append(perturbed_img)\n",
        "        misclassified_true_images.append(cur_PIL)\n",
        "        perturbations.append(json_list)\n",
        "\n",
        "      similarity_score = image_distance(cur_PIL, perturbed_img, clip_model,\n",
        "                                        clip_preprocess)\n",
        "\n",
        "      one_hot_encoding_of_label = torch.zeros(10, device=device)\n",
        "      one_hot_encoding_of_label[probabilities.argmax()] = 1\n",
        "      loss = loss_func(one_hot_encoding_of_label, probabilities,\n",
        "                        similarity_score)\n",
        "      # loss = loss_func(label, probabilities, similarity_score)\n",
        "      # Don't feed error examples back (Might want to instead feed them as \"Bad\" examples)\n",
        "      json_list = [json_list[i] for i in range(len(json_list)) if i not in remove_indices]\n",
        "      print(f\"Current Loss: {loss}\")\n",
        "\n",
        "      # Generate new prompt\n",
        "      prompt = generate_new_perturbations_prompt(json.dumps(json_list), loss,\n",
        "                                                  perturbed_img.size)\n",
        "\n",
        "      # Update current_img count\n",
        "      current_img += 1\n",
        ""
      ],
      "metadata": {
        "id": "djcs4FYWFyhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9641a9-0480-474c-ce8a-3460183aaf14"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING EPOCH: 0\n",
            "Obtaining perturbations for image #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying perturbations...\n",
            "apply perturbation #0 for image #0\n",
            "apply perturbation #1 for image #0\n",
            "apply perturbation #2 for image #0\n",
            "apply perturbation #3 for image #0\n",
            "apply perturbation #4 for image #0\n",
            "Error in operation: add_patch\n",
            "{'operation': 'add_patch', 'location': [150, 150], 'size': [50, 50], 'type': 'noise'}\n",
            "apply perturbation #4 for image #0\n",
            "Error in operation: add_patch\n",
            "{'operation': 'add_patch', 'location': [80, 80], 'size': [40, 40], 'type': 'color', 'color': [0, 255, 0]}\n",
            "apply perturbation #4 for image #0\n",
            "cuda\n",
            "Correctly classified perturbed image 0\n",
            "Current Loss: tensor([1.3969], device='cuda:0')\n",
            "Obtaining perturbations for image #1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying perturbations...\n",
            "apply perturbation #0 for image #1\n",
            "apply perturbation #1 for image #1\n",
            "apply perturbation #2 for image #1\n",
            "apply perturbation #3 for image #1\n",
            "apply perturbation #4 for image #1\n",
            "apply perturbation #5 for image #1\n",
            "apply perturbation #6 for image #1\n",
            "Error in operation: add_patch\n",
            "{'operation': 'add_patch', 'location': [20, 20], 'size': 15, 'type': 'circle', 'color': [0.8, 0.2, 0.2]}\n",
            "apply perturbation #6 for image #1\n",
            "apply perturbation #7 for image #1\n",
            "apply perturbation #8 for image #1\n",
            "cuda\n",
            "Classified perturbed image 1 wrong\n",
            "Current Loss: tensor([1.8256], device='cuda:0')\n",
            "Obtaining perturbations for image #2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying perturbations...\n",
            "apply perturbation #0 for image #2\n",
            "apply perturbation #1 for image #2\n",
            "apply perturbation #2 for image #2\n",
            "apply perturbation #3 for image #2\n",
            "apply perturbation #4 for image #2\n",
            "apply perturbation #5 for image #2\n",
            "apply perturbation #6 for image #2\n",
            "Error in operation: add_patch\n",
            "{'operation': 'add_patch', 'location': [10, 10], 'size': 25, 'type': 'gaussian', 'color': 'blue'}\n",
            "apply perturbation #6 for image #2\n",
            "apply perturbation #7 for image #2\n",
            "apply perturbation #8 for image #2\n",
            "cuda\n",
            "Classified perturbed image 2 wrong\n",
            "Current Loss: tensor([1.9712], device='cuda:0')\n",
            "Obtaining perturbations for image #3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying perturbations...\n",
            "apply perturbation #0 for image #3\n",
            "apply perturbation #1 for image #3\n",
            "apply perturbation #2 for image #3\n",
            "apply perturbation #3 for image #3\n",
            "apply perturbation #4 for image #3\n",
            "apply perturbation #5 for image #3\n",
            "apply perturbation #6 for image #3\n",
            "apply perturbation #7 for image #3\n",
            "apply perturbation #8 for image #3\n",
            "apply perturbation #9 for image #3\n",
            "Error in operation: add_patch\n",
            "{'operation': 'add_patch', 'location': [5, 5], 'size': 25, 'type': 'gaussian', 'color': [0.8, 0.2, 0.2]}\n",
            "cuda\n",
            "Classified perturbed image 3 wrong\n",
            "Current Loss: tensor([1.5095], device='cuda:0')\n",
            "Obtaining perturbations for image #4\n",
            "Applying perturbations...\n",
            "apply perturbation #0 for image #4\n",
            "apply perturbation #1 for image #4\n",
            "apply perturbation #2 for image #4\n",
            "apply perturbation #3 for image #4\n",
            "apply perturbation #4 for image #4\n",
            "apply perturbation #5 for image #4\n",
            "apply perturbation #6 for image #4\n",
            "apply perturbation #7 for image #4\n",
            "apply perturbation #8 for image #4\n",
            "apply perturbation #9 for image #4\n",
            "Error in operation: add_patch\n",
            "{'operation': 'add_patch', 'location': [15, 15], 'size': 30, 'type': 'gaussian', 'color': [0.5, 0.5, 0.5]}\n",
            "cuda\n",
            "Classified perturbed image 4 wrong\n",
            "Current Loss: tensor([1.5737], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check final accuracy of inferences\n",
        "print(f\"Baseline Accuracy (from fine-tuning): 93.41%\")\n",
        "print(f\"Final Accuracy: {(100 * (correct/len(test)))}%\")"
      ],
      "metadata": {
        "id": "tj6TjlBJXvzk",
        "outputId": "07b728f7-f0c0-4b75-c02a-1a6ca9745abe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy (from fine-tuning): 93.41%\n",
            "Final Accuracy: 0.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(misclassified_images[0])"
      ],
      "metadata": {
        "id": "hKkDtHzxW2ak",
        "outputId": "3c6c316f-38ec-427e-ece6-4fec8b883392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7da6d314cb50>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(misclassified_true_images[0])"
      ],
      "metadata": {
        "id": "pKUy5O74YjjM",
        "outputId": "36b7dcae-0b77-44d7-93ac-b57d23f2e306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7da6d32fecd0>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}